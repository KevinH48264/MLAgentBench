{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New working directory:  c:\\Users\\kevihuang\\OneDrive - Microsoft\\Desktop\\projects\\MLAgentBench\\MLAgentBench_v2\\agents\\Voyager\\..\\..\\..\n",
      "New Working Directory: c:\\Users\\kevihuang\\OneDrive - Microsoft\\Desktop\\projects\\MLAgentBench\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the new working directory relative to the current working directory\n",
    "# or use an absolute path\n",
    "new_working_directory = os.path.join(os.getcwd(), '..', '..', '..') # Set to MLAgentBenhc\n",
    "print(\"New working directory: \", new_working_directory)\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(new_working_directory)\n",
    "\n",
    "print(\"New Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'helm'\n",
      "Could not load CRFM API key crfm_api_key.txt.\n",
      "[Errno 2] No such file or directory: 'claude_api_key.txt'\n",
      "Could not load anthropic API key claude_api_key.txt.\n"
     ]
    }
   ],
   "source": [
    "from MLAgentBench_v2.LLM import complete_text_openai\n",
    "from MLAgentBench_v2.agents.agent import Agent\n",
    "import numpy as np \n",
    "import json\n",
    "import logging \n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time \n",
    "import types\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing environment...\n",
      "args namespace(task='home-data-for-ml-course', task_type='kaggle', log_dir='logs/house-price-testing_simple-assistant-agent_gpt35_v1', work_dir='workspace', max_steps=50, max_time=18000, device=0, python='/home/user/micromamba/envs/autogpt/bin/python', interactive=False, resume=None, resume_step=0, agent_type='SimpleAssistantAgent', llm_name='gpt-3.5-turbo-1106', fast_llm_name='gpt-3.5-turbo-1106', edit_script_llm_name='gpt-3.5-turbo-1106', edit_script_llm_max_tokens=4000, agent_max_steps=50, actions_remove_from_prompt=[], actions_add_to_prompt=[], no_retrieval=False, valid_format_entires=None, max_steps_in_context=3, max_observation_steps_in_context=3, max_retries=4, langchain_agent='zero-shot-react-description')\n",
      "Preparing task home-data-for-ml-course , of type:  kaggle\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an environment\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from MLAgentBench_v2.environment import Environment\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    task='home-data-for-ml-course',\n",
    "    task_type='kaggle',\n",
    "    log_dir='logs/house-price-testing_simple-assistant-agent_gpt35_v1',\n",
    "    work_dir='workspace',\n",
    "    max_steps=50,\n",
    "    max_time=18000,\n",
    "    device=0,\n",
    "    python='/home/user/micromamba/envs/autogpt/bin/python',\n",
    "    interactive=False,\n",
    "    resume=None,\n",
    "    resume_step=0,\n",
    "    agent_type='SimpleAssistantAgent',\n",
    "    llm_name='gpt-3.5-turbo-1106',\n",
    "    fast_llm_name='gpt-3.5-turbo-1106',\n",
    "    edit_script_llm_name='gpt-3.5-turbo-1106',\n",
    "    edit_script_llm_max_tokens=4000,\n",
    "    agent_max_steps=50,\n",
    "    actions_remove_from_prompt=[],\n",
    "    actions_add_to_prompt=[],\n",
    "    no_retrieval=False,\n",
    "    valid_format_entires=None,\n",
    "    max_steps_in_context=3,\n",
    "    max_observation_steps_in_context=3,\n",
    "    max_retries=4,\n",
    "    langchain_agent='zero-shot-react-description'\n",
    ")\n",
    "\n",
    "env = Environment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_file_directory = \"workspace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt for generating curriculum: \n",
      " You are a helpful assistant that tells me the next immediate task to do. My ultimate goal is to discover as many useful pieces of information as possible to better achieve the research goal, answer as many questions as possible to get the best answer, and become the best researcher in the world in solving this research goal.\n",
      "\n",
      "        Research Goal: Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n",
      "\n",
      "With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n",
      "\n",
      "Evaluation\n",
      "Goal\n",
      "It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. You want a train and validation MAE of lower than 15,000 and there should be a submission.csv containing predictions for test.csv ready to submit.\n",
      "\n",
      "Metric\n",
      "Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n",
      "\n",
      "Submission File Format\n",
      "The file should contain a header and have the following format:\n",
      "\n",
      "Id,SalePrice\n",
      "1461,169000.1\n",
      "1462,187724.1233\n",
      "1463,175221\n",
      "etc.\n",
      "\n",
      "        I will give you the following information:\n",
      "        Skills: these are skills that I can take action with.\n",
      "        Files: these are my current files that I have in my working directory.\n",
      "        Most recent files, action, result, and answer states (oldest to newest): Answer states are the a report of the best answer I have so far to achieving the research goal, and the files, action, and result are the files, action, and result I took at that point in time to produce the answer state.    \n",
      "        Completed tasks so far: ...\n",
      "        Failed tasks that are too hard: ...\n",
      "\n",
      "        1) You should act as a mentor and guide me to the next task based on my current learning progress.\n",
      "        2) Please be very specific about what information or actions I need to take and what expected results I need to achieve.\n",
      "        3) The next task should follow a clear format, such as \"Write [file]\", \"Reflect on why I'm seeing [error]\", \"Read [file]\", \"Brainstorm if ideas from [topic 1] be applied to [topic 2]\", \"Analyze what are the similarities between [topic 1] for success and [topic 2]\" , \"Reflect on what's significant about this paper: [paper]?\", \"Reflect on what's currently missing or preventing me from achieving [goal] better\", etc. It should be a single task to collect useful information on. Do not propose multiple tasks at the same time. Do not mention anything else. \n",
      "        4) The next task should not be too hard since the internet and I may not contain the full answer in a single article or have learned enough information to complete it yet. \n",
      "        5) The next task should be novel and interesting based on my current learning progress. I should look for rare and potentially useful pieces of information, upgrade my current answer using better information, and discover new things. I should not be doing the same thing over and over again.\n",
      "        6) I may sometimes need to repeat some tasks or variations of the task if I need to collect more information to answer more difficult tasks. Only repeat tasks if necessary. \n",
      "        7) I want to explore the world and discover new things. I don’t want to stay with my current answer for too long. \n",
      "        8) Tasks that require information beyond another reader's ability to theoretically verify and reason if completed or correct should be avoided. For instance, \"what else is there on the website?\" and \"what images and tables are on the website\" are not ideal since they require visual confirmation from the screen. All the testing, coding, and asking other people questions should be avoided. Do not propose a task  with these keywords. You should only respond in the format as described below:\n",
      "        \n",
      "        RESPONSE FORMAT: \n",
      "        ```json\n",
      "        { \n",
      "            \"reasoning\": \"<based on the information I listed above, do reasoning about what the next task should be.>\",\n",
      "            \"task\": \"<the next task.>\"\n",
      "        }\n",
      "        ```\n",
      "        \n",
      "        Here’s an example response: \n",
      "        ```json\n",
      "        { \n",
      "            \"reasoning\": \"We know that we have a sword and we know there's fire, and fire lights things on fire. Therefore, we could try to make a firesword.\",\n",
      "            \"task\": \"Try to make a firesword and record what happens.\"\n",
      "        }\n",
      "        ```\n",
      "\n",
      "        Ensure the response can be parsed by Python \"json.loads\", e.g.: no trailing commas, no single quotes, etc. This is important.\n",
      " \n",
      " User prompt:  Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Skills: ['reflection', 'readFile', 'writeFile', 'executeScript']\n",
      "Most recent files, action, result, and answer states (oldest to newest):\n",
      "\n",
      "Step: 0\n",
      "Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: None\n",
      "Result: None\n",
      "Answer: None      \n",
      "Completed tasks so far: []\n",
      "Failed tasks that are too hard: []\n",
      "\n",
      "In complete_text_openai(), Completion JSON:  {'reasoning': \"Since you have the dataset files, it would be beneficial to start by reading the 'data_description.txt' file to understand the meaning of each variable in the dataset. This will provide valuable insights into the features that may influence the sale price of the houses.\", 'task': \"Read 'data_description.txt' to understand the meaning of each variable in the dataset.\"}\n",
      "Response:  {\n",
      "  \"reasoning\": \"Since you have the dataset files, it would be beneficial to start by reading the 'data_description.txt' file to understand the meaning of each variable in the dataset. This will provide valuable insights into the features that may influence the sale price of the houses.\",\n",
      "  \"task\": \"Read 'data_description.txt' to understand the meaning of each variable in the dataset.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class CurriculumAgent(Agent):\n",
    "    def __init__(self, env, completed_tasks=[], failed_tasks=[]):\n",
    "        super().__init__(env)\n",
    "        self.completed_tasks = completed_tasks\n",
    "        self.failed_tasks = failed_tasks\n",
    "\n",
    "        # The crux is a Q&A process\n",
    "        # Problem with this approach is you still have to deal with searching multiple times, and continuing to search or not. Approach: Or maybe if you search and you don't have the answer, that's a bad thing to search and you need to go more specific / ask a different question!\n",
    "        self.system_prompt_automatic_curriculum = f'''You are a helpful assistant that tells me the next immediate task to do. My ultimate goal is to discover as many useful pieces of information as possible to better achieve the research goal, answer as many questions as possible to get the best answer, and become the best researcher in the world in solving this research goal.\n",
    "\n",
    "        Research Goal: {self.research_problem}\n",
    "\n",
    "        I will give you the following information:\n",
    "        Skills: these are skills that I can take action with.\n",
    "        Files: these are my current files that I have in my working directory.\n",
    "        Most recent files, action, result, and answer states (oldest to newest): Answer states are the a report of the best answer I have so far to achieving the research goal, and the files, action, and result are the files, action, and result I took at that point in time to produce the answer state.    \n",
    "        Completed tasks so far: ...\n",
    "        Failed tasks that are too hard: ...\n",
    "\n",
    "        1) You should act as a mentor and guide me to the next task based on my current learning progress.\n",
    "        2) Please be very specific about what information or actions I need to take and what expected results I need to achieve.\n",
    "        3) The next task should follow a clear format, such as \"Write [file]\", \"Reflect on why I'm seeing [error]\", \"Read [file]\", \"Brainstorm if ideas from [topic 1] be applied to [topic 2]\", \"Analyze what are the similarities between [topic 1] for success and [topic 2]\" , \"Reflect on what's significant about this paper: [paper]?\", \"Reflect on what's currently missing or preventing me from achieving [goal] better\", etc. It should be a single task to collect useful information on. Do not propose multiple tasks at the same time. Do not mention anything else. \n",
    "        4) The next task should not be too hard since the internet and I may not contain the full answer in a single article or have learned enough information to complete it yet. \n",
    "        5) The next task should be novel and interesting based on my current learning progress. I should look for rare and potentially useful pieces of information, upgrade my current answer using better information, and discover new things. I should not be doing the same thing over and over again.\n",
    "        6) I may sometimes need to repeat some tasks or variations of the task if I need to collect more information to answer more difficult tasks. Only repeat tasks if necessary. \n",
    "        7) I want to explore the world and discover new things. I don’t want to stay with my current answer for too long. \n",
    "        8) Tasks that require information beyond another reader's ability to theoretically verify and reason if completed or correct should be avoided. For instance, \"what else is there on the website?\" and \"what images and tables are on the website\" are not ideal since they require visual confirmation from the screen. All the testing, coding, and asking other people questions should be avoided. Do not propose a task  with these keywords. You should only respond in the format as described below:\n",
    "        \n",
    "        RESPONSE FORMAT: \n",
    "        ```json\n",
    "        {{ \n",
    "            \"reasoning\": \"<based on the information I listed above, do reasoning about what the next task should be.>\",\n",
    "            \"task\": \"<the next task.>\"\n",
    "        }}\n",
    "        ```\n",
    "        \n",
    "        Here’s an example response: \n",
    "        ```json\n",
    "        {{ \n",
    "            \"reasoning\": \"We know that we have a sword and we know there's fire, and fire lights things on fire. Therefore, we could try to make a firesword.\",\n",
    "            \"task\": \"Try to make a firesword and record what happens.\"\n",
    "        }}\n",
    "        ```\n",
    "\n",
    "        Ensure the response can be parsed by Python \"json.loads\", e.g.: no trailing commas, no single quotes, etc. This is important.\n",
    "'''\n",
    "\n",
    "        # TODO: This is optional, might be useful, but to focus on a system prompt of asking questions and answering questions.\n",
    "        # System 2: this is a more scoped down version where we have the focus be on only answering questions -- reading and analyzing information & asking questions. No action items. \n",
    "        # The current above system 1 is better for self-driving labs type of work where there are going to be more tasks.\n",
    "\n",
    "    def get_exploration_progress(self):\n",
    "        # TODO: this should contain inventory of where we're at now and what files we have / memory stream\n",
    "        return f'''Completed tasks: {self.completed_tasks}, Failed tasks: {self.failed_tasks}'''\n",
    "\n",
    "    def propose_next_task(self):\n",
    "        '''\n",
    "        This function decomposes a goal into tasks\n",
    "        '''        \n",
    "        user_prompt = f'''Files: {self.files}\n",
    "Skills: {list(self.available_actions.keys())}\n",
    "Most recent files, action, result, and answer states (oldest to newest):\n",
    "{self.formatted_answer_states}      \n",
    "Completed tasks so far: {self.completed_tasks}\n",
    "Failed tasks that are too hard: {self.failed_tasks}\n",
    "'''\n",
    "        \n",
    "        print(\"System prompt for generating curriculum: \\n\", self.system_prompt_automatic_curriculum, \"\\n User prompt: \", user_prompt)\n",
    "        next_task_response = complete_text_openai(user_prompt, system_prompt=self.system_prompt_automatic_curriculum, json_required=True)\n",
    "        print(\"Response: \", next_task_response)\n",
    "        next_task = json.loads(next_task_response)[\"task\"]\n",
    "        return next_task\n",
    "\n",
    "    def get_completed_tasks():\n",
    "        pass\n",
    "\n",
    "    def get_failed_tasks():\n",
    "        pass\n",
    "\n",
    "    def add_completed_task(self, task):\n",
    "        # TODO: probably we should record the entire answer state of files, action, output, and answer state? Or just action and output?\n",
    "        self.completed_tasks.append(task)\n",
    "\n",
    "    def add_failed_task(self, task):\n",
    "        self.failed_tasks.append(task)\n",
    "\n",
    "curriculum_agent = CurriculumAgent(env)\n",
    "next_task = curriculum_agent.propose_next_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available functions: \n",
      " [{'type': 'function', 'function': {'name': 'reflection', 'description': 'Use this to reflect on all past steps. Provide a detailed description on what to reflect on and what should be returned.', 'parameters': {'type': 'object', 'properties': {'things_to_reflect_on': {'type': 'string', 'description': 'A detailed description on what to reflect on and what should be returned'}}, 'required': ['things_to_reflect_on']}}}, {'type': 'function', 'function': {'name': 'readFile', 'description': 'Use this to read an existing file.', 'parameters': {'type': 'object', 'properties': {'file_name': {'type': 'string', 'description': 'A valid file name with relative path to current directory if needed'}}, 'required': ['file_name']}}}, {'type': 'function', 'function': {'name': 'writeFile', 'description': 'Use this to write a file. If the file already exists, it will be overwritten.', 'parameters': {'type': 'object', 'properties': {'file_name': {'type': 'string', 'description': 'A valid file name with relative path to current directory if needed'}, 'content': {'type': 'string', 'description': 'The content to be written to the file. Please know that the execute script function will execute from the same current working directory. Also that execute script will only output the stdout of the script, so do not use visualizations or other outputs that are not stdout.'}}, 'required': ['file_name', 'content']}}}, {'type': 'function', 'function': {'name': 'executeScript', 'description': 'Use this to execute the python script. The script must already exist.', 'parameters': {'type': 'object', 'properties': {'script_name': {'type': 'string', 'description': 'A valid python script name with relative path to current directory if needed. You can only execute scripts and files in the current directory.'}}, 'required': ['script_name']}}}] \n",
      "\n",
      "Functions ['reflection', 'readFile', 'writeFile', 'executeScript'] \n",
      "\n",
      "Available files: \n",
      " ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "class SkillManager(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def retrieve_skills(self, task, execution_feedback):\n",
    "        # For sake of simplicity, use recency for now (later relevancy and importance can be added). TODO: leverage task and execution feedback to guide what action_history memories to retrieve\n",
    "        func_name_description_list = list(self.available_actions.keys())\n",
    "        return func_name_description_list\n",
    "    \n",
    "    # Taking this out for now to just leave the base actions intact because other actions like training a model will be a file that can be retrieved as part of memory for the curriculum agent and not the skill agent\n",
    "    # def retrieve_info_blocks(self, task, execution_feedback):\n",
    "    #     # retrieving file names and description\n",
    "    #     return [name + \" - \" + description for name, description in self.files]\n",
    "\n",
    "    # Helper function: be able to create a function with a dynamic name and return value\n",
    "    # def create_skill_function(self, function_name, return_value):\n",
    "    #     def dynamic_method(self):\n",
    "    #         return return_value\n",
    "    #     # Bind the function to the instance as a method\n",
    "    #     bound_method = types.MethodType(dynamic_method, self)\n",
    "    #     setattr(self, function_name, bound_method)\n",
    "    #     # Add the method to available functions\n",
    "    #     self.available_functions[function_name] = bound_method\n",
    "\n",
    "    # Core function: adding a new skill requires an original task, a validated answer, and a message history\n",
    "    # def add_skill(self, task, validated_answer, methods_prompt):\n",
    "    #     # TODO: wait until the action agent generates a function because maybe you only need to write a description of the input function instead of task and validated answer.\n",
    "    #     # create_function_description_system_prompt = f'''You are a helpful assistant that writes a description of the given '''\n",
    "\n",
    "    #     print(\"Adding skills! \", task, validated_answer, methods_prompt)\n",
    "\n",
    "    #     create_skill_system_prompt = f'''You are a helpful assistant. Your goal is to write a short file name and a short description of the task and answer. \n",
    "        \n",
    "    #     You will receive this information:\n",
    "    #     Original task or question: ...\n",
    "    #     Answer: ...\n",
    "\n",
    "    #     Do not use any of these file names: {[name for name, _ in self.files]}\n",
    "\n",
    "    #     Your output should be in the following format if function requires arguments:\n",
    "    #     ```json\n",
    "    #     {{\n",
    "    #         \"name\": \"<file_name>\",\n",
    "    #         \"description\": \"<insert question and answer>\"\n",
    "    #     }}\n",
    "    #     ```\n",
    "\n",
    "    #     Good example output:\n",
    "    #     ```json\n",
    "    #     {{\n",
    "    #         \"name\": \"num_dogs_in_bens_family\",\n",
    "    #         \"description\": \"The question was how many dogs are in the family. Ben said that he has 2 dogs in his family.\"\n",
    "    #     }}\n",
    "    #     ```\n",
    "\n",
    "    #     Ensure the response can be parsed by Python \"json.loads\", e.g.: no trailing commas, no single quotes, etc. This is important.\n",
    "    #     '''\n",
    "\n",
    "    #     create_function_description_prompt = f'''\n",
    "    #     Original task or question: {task}\n",
    "    #     Answer: {validated_answer}\n",
    "    #     '''\n",
    "    #     res, messages = complete_text_openai(prompt=create_function_description_prompt, system_prompt=create_skill_system_prompt, verbose=True)\n",
    "    #     res\n",
    "\n",
    "    #     try:\n",
    "    #         # Load the function description\n",
    "    #         file_name_description = json.loads(res['content'])\n",
    "    #         print(\"file_name_description: \", file_name_description)\n",
    "\n",
    "    #         # Create the function as a method of skill_manager\n",
    "    #         self.write_file(file_name_description['name'], f\"Question: {task}\\nAnswer: {validated_answer}\\nReasoning and Methods: {methods_prompt}\")\n",
    "\n",
    "    #         # Add function to function description list\n",
    "    #         self.files.append((file_name_description['name'], file_name_description['description']))\n",
    "\n",
    "    #         print(\"COMPLETE!\")\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"An unexpected error occurred: {e}\")\n",
    "    #         return\n",
    "\n",
    "    #     return\n",
    "    \n",
    "skill_manager = SkillManager(env)\n",
    "print(\"Available functions: \\n\", skill_manager.tool_descriptions, \"\\n\\nFunctions\", list(skill_manager.available_actions.keys()), \"\\n\\nAvailable files: \\n\", skill_manager.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "methods_agent_feedback Plan:\n",
      "To complete the task of understanding the meaning of each variable in the dataset, we can follow these steps:\n",
      "\n",
      "Steps:\n",
      "1) Read the contents of the file 'data_description.txt' to understand the meaning of each variable in the dataset.\n",
      "2) Reflect on the information obtained from the file to gain a clear understanding of the variables and their meanings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Task: Read 'data_description.txt' to understand the meaning of each variable in the dataset.\\nInstructions: Plan:\\nTo complete the task of understanding the meaning of each variable in the dataset, we can follow these steps:\\n\\nSteps:\\n1) Read the contents of the file 'data_description.txt' to understand the meaning of each variable in the dataset.\\n2) Reflect on the information obtained from the file to gain a clear understanding of the variables and their meanings.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MethodsAgent(Agent):\n",
    "    # TODO: does there need to be an action agent generating the steps? Or does there need to be a separate execution agent running the prompt? Or can the action agent be the execution agent?\n",
    "    # TODO: can a critic agent really check if the output is correct? Or can they only check if that aligns with expectation? Otherwise, the critic will have to check the line of content values to make sure the reasoning is sound, which is still doable, but the extent that another critic can check is limited. I guess it's just to make sure the reasoning is sound.\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def generate_function_callable_prompt(self, task,\n",
    "                methods_prompt,\n",
    "                execution_feedback,\n",
    "                execution_errors,\n",
    "                critique,\n",
    "                skills):\n",
    "        generate_plan_prompt = '''You are a helpful assistant and a first-rate problem solver. Given a task or question, your goal is to list out the steps to solve that task given your skills and reasoning. Ultimately, your output should be able to be followed by a human limited by the skills and knowledge given, and another human should be able to check that human's output to see if it's correct and reasonable. Note that the functions asked for may sometimes already be called and the information from the function that you need is already in the prompt, so read carefully. Note that you DO NOT have the ability to see, you can only read, think, write, and execute scripts using the existing skills and knowledge.\n",
    "        \n",
    "        You will be given this information:\n",
    "        Task or question: ...\n",
    "        Skills: these are skills that I can take action with.\n",
    "        Files: these are my current files that I have in my working directory.\n",
    "        History of files, action, and result (oldest to newest): By following the plan, this is my history of files, action, and result I had and took at that point in time.   \n",
    "        Current state plan: ...\n",
    "        Current state output after executing steps: ...\n",
    "        Execution errors: ...\n",
    "        Critique: ...\n",
    "        \n",
    "        You should then respond to me with\n",
    "        Explain (if applicable): Are there any steps missing in your plan? Why do the current state steps not complete the task? What do the current state output, execution errors, and critique imply?\n",
    "        Plan: How to complete the task step by step. You should pay attention and read Files because it tells you what information you have. The task completeness check is also based on your final answer state and final files.\n",
    "        Steps: \n",
    "        1) Write how to complete the task step by step. \n",
    "        2) Reuse the above useful files as much as possible.\n",
    "        3) Your task completion and plan will be reused to achieving more complex tasks. Thereofre, you should make it generic and reusable. You should not make strong assumptions about the files (as it may be changed at a later time), and therefore you should always check whether you have the required files before using them. If not, you should first create the required files, get the necessary information, and reuse the above useful actions.\n",
    "        '''\n",
    "\n",
    "        user_prompt = f'''Task: {task}\n",
    "        Files: {self.files}\n",
    "        Skills: {list(self.available_actions.keys())}\n",
    "        History of files, action, and result:\n",
    "        {self.formatted_action_history}    \n",
    "        Skills: {skills}\n",
    "        Current state plan: {methods_prompt}\n",
    "        Current state output after executing steps: {execution_feedback}\n",
    "        Execution errors: {execution_errors}\n",
    "        Critique: {critique}'''\n",
    "\n",
    "        methods_agent_feedback = complete_text_openai(user_prompt, system_prompt=generate_plan_prompt)\n",
    "        # TODO: parse out just the steps to save\n",
    "        # agent_methods_feedback_json = json.loads(agent_methods_feedback['content'])\n",
    "\n",
    "        print(\"methods_agent_feedback\", methods_agent_feedback)\n",
    "\n",
    "        return \"Task: \" + task + \"\\nInstructions: \" + methods_agent_feedback\n",
    "    \n",
    "methods_agent = MethodsAgent(env)\n",
    "methods_prompt = None\n",
    "execution_feedback = None\n",
    "execution_errors = None\n",
    "critique = None\n",
    "success = False\n",
    "methods_agent.generate_function_callable_prompt(task=next_task, methods_prompt=methods_prompt, execution_feedback=execution_feedback, execution_errors=execution_errors, critique=critique, skills=list(skill_manager.available_actions.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExecutionAgent(Agent):\n",
    "    # Ultimately the answer will go into Description or be \"returned\" with the description being condensed into a tldr and the methods_prompt will be added to history\n",
    "\n",
    "    # TODO: Perhaps the execution agent can get info about the files too, but maybe that's the action agent's responsibility.\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def function_call(self, methods_prompt):\n",
    "        system_prompt = f'''You are a helpful assistant. Your goal is to execute the given instructions and output the complete answer to the question. If the instructions don't seem reasonable or you cannot get to the complete answer, then you should give feedback on why you couldn't do it and what you tried. \n",
    "\n",
    "        You will be given this information:\n",
    "        Skills: these are skills that I can take action with.\n",
    "        Files: these are my current files that I have in my working directory.\n",
    "        History of files, action, and result (oldest to newest): By following the plan, this is my history of files, action, and result I had and took at that point in time.\n",
    "        Task: ...\n",
    "        Instructions: ...\n",
    "'''\n",
    "        execute_prompt = f'''Skills: {list(self.available_actions.keys())}\n",
    "        Files: {self.files}\n",
    "        History of files, action, and result:\n",
    "        {self.formatted_action_history}\n",
    "        {methods_prompt}'''\n",
    "\n",
    "        num_states = len(self.formatted_action_history)\n",
    "        try:\n",
    "            # complete_text_openai(system_prompt=system_prompt, prompt=execute_prompt, tools=self.tool_descriptions, available_functions=self.available_actions) # Normal function calling\n",
    "            self.run_assistant(system_prompt=system_prompt, user_prompt=execute_prompt) # OpenAI Assistants API\n",
    "        except Exception as e:\n",
    "            return \"\"\n",
    "        return self.formatted_action_history # Answer state will save all actions taken\n",
    "    \n",
    "    # Function to run the OpenAI Assistants API\n",
    "    def run_assistant(self, system_prompt, user_prompt):\n",
    "        # Instantiate an Assistant\n",
    "        self.assistant = self.client.beta.assistants.create(\n",
    "            name=\"Research Agent\",\n",
    "            instructions=system_prompt,\n",
    "            tools=self.tool_descriptions,\n",
    "            model=self.model\n",
    "        )\n",
    "        self.thread = self.client.beta.threads.create()\n",
    "\n",
    "        # Invoke the Assistants API to answer\n",
    "        self.client.beta.threads.messages.create(\n",
    "            thread_id=self.thread.id,\n",
    "            role=\"user\",\n",
    "            content=user_prompt\n",
    "        )\n",
    "        run = self.client.beta.threads.runs.create(\n",
    "            thread_id=self.thread.id,\n",
    "            assistant_id=self.assistant.id,\n",
    "        )\n",
    "\n",
    "        # Wait until the run has looped\n",
    "        run_complete = False\n",
    "        num_tries = 100\n",
    "        while not run_complete:\n",
    "            # Check if there's an update on the run\n",
    "            run = self.client.beta.threads.runs.retrieve(\n",
    "                thread_id=self.thread.id,\n",
    "                run_id=run.id\n",
    "            )\n",
    "            run_complete = run.status == \"completed\"\n",
    "            print(\"\\nrun.status: \", run.status)\n",
    "\n",
    "            # Call the tools if the run status is requires action\n",
    "            if run.status == \"requires_action\":\n",
    "                tool_outputs = []\n",
    "                for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n",
    "                    tool_id, tool_function, tool_type = tool_call.id, tool_call.function, tool_call.type\n",
    "                    print(f\"Run required action: \\ntool_id: {tool_id}, \\ntool_function.arguments: {tool_function.arguments}, \\ntool_function.name: {tool_function.name}, \\ntool_type: {tool_type}\")\n",
    "\n",
    "                    # Call the function directly if `tool_function` is a callable object\n",
    "                    # and `arguments` is a dictionary of arguments to pass to the function.\n",
    "                    try:\n",
    "                        arguments = json.loads(tool_function.arguments)\n",
    "                        function_output = self.available_actions[tool_function.name](**arguments)\n",
    "                    except Exception as e:\n",
    "                        function_output = f\"Tool function {tool_function.name} for tool_id {tool_id} does not exist and is not callable with arguments {tool_function.arguments}. Make sure you are using only tools listed here: {self.available_actions.keys()} with the right arguments.\"\n",
    "\n",
    "                    tool_outputs.append({\n",
    "                        \"tool_call_id\": tool_id,\n",
    "                        \"output\": function_output\n",
    "                    })\n",
    "\n",
    "                # Submit tool outputs as a new run\n",
    "                run = self.client.beta.threads.runs.submit_tool_outputs(\n",
    "                    thread_id=self.thread.id,\n",
    "                    run_id=run.id,\n",
    "                    tool_outputs=tool_outputs\n",
    "                )\n",
    "            elif run.status == \"failed\":\n",
    "                print(\"Run failed: \", run)\n",
    "                break\n",
    "\n",
    "            time.sleep(1)\n",
    "            num_tries -= 1\n",
    "            if num_tries == 0:\n",
    "                print(\"Run timed out, cancelling...\")\n",
    "                run = self.client.beta.threads.runs.cancel(thread_id=self.thread.id, run_id=run.id)\n",
    "                while run.status != \"cancelled\":\n",
    "                    run = self.client.beta.threads.runs.retrieve(\n",
    "                        thread_id=self.thread.id,\n",
    "                        run_id=run.id\n",
    "                    )\n",
    "                print(\"Run cancelled!\")\n",
    "                break\n",
    "        messages = self.client.beta.threads.messages.list(thread_id=self.thread.id)\n",
    "        completion = messages.data[0].content[0].text.value\n",
    "        return completion\n",
    "execution_agent = ExecutionAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "run.status:  queued\n",
      "\n",
      "run.status:  in_progress\n",
      "\n",
      "run.status:  in_progress\n",
      "\n",
      "run.status:  requires_action\n",
      "Run required action: \n",
      "tool_id: call_x2TjTV3JoFuHOSWO1mqG7sIi, \n",
      "tool_function.arguments: {\"file_name\":\"research_plan.txt\",\"content\":\"Research Plan:\\n1. Explore the dataset files 'data_description.txt', 'train.csv', and 'test.csv' to understand the features and the structure of the data.\\n2. Preprocess the data, handle missing values, and perform feature engineering to prepare it for model training.\\n3. Choose and implement machine learning models to make predictions. Split the training data into a training set and a validation set to evaluate the model's performance.\\n4. Use techniques such as cross-validation and hyperparameter tuning to optimize the model's performance.\\n5. Make predictions on the test set and create a submission file in the specified format.\\n6. Ensure that the predictions meet the required criteria, such as a train and validation Mean Absolute Error (MAE) of lower than 15,000.\\n7. Seek help from the competition's community forums, explore relevant resources, or consult with experienced data scientists if encountering any challenges or uncertainties during the process.\\nGood luck with your research!\"}, \n",
      "tool_function.name: writeFile, \n",
      "tool_type: function\n",
      "\n",
      "Step: 4\n",
      "Calling function wrapped_write_file(args = (), kwargs = {'file_name': 'research_plan.txt', 'content': \"Research Plan:\\n1. Explore the dataset files 'data_description.txt', 'train.csv', and 'test.csv' to understand the features and the structure of the data.\\n2. Preprocess the data, handle missing values, and perform feature engineering to prepare it for model training.\\n3. Choose and implement machine learning models to make predictions. Split the training data into a training set and a validation set to evaluate the model's performance.\\n4. Use techniques such as cross-validation and hyperparameter tuning to optimize the model's performance.\\n5. Make predictions on the test set and create a submission file in the specified format.\\n6. Ensure that the predictions meet the required criteria, such as a train and validation Mean Absolute Error (MAE) of lower than 15,000.\\n7. Seek help from the competition's community forums, explore relevant resources, or consult with experienced data scientists if encountering any challenges or uncertainties during the process.\\nGood luck with your research!\", 'work_dir': 'workspace\\\\home-data-for-ml-course_branch'})\n",
      "\n",
      "--- TOOL SUCCESS ---\n",
      "Finished!\n",
      "\n",
      "run.status:  completed\n",
      "Execution feedback \n",
      "Step: 0\n",
      "Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: None\n",
      "Result: None\n",
      "Step: 1\n",
      "Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: Calling function wrapped_reflection(args = (), kwargs = {'things_to_reflect_on': \"I'm not sure what action to take next. I need more information to proceed.\", 'work_dir': '.'})\n",
      "Result: Reflection: It seems like you are participating in a data science competition to predict the sales price of residential homes in Ames, Iowa. The dataset contains 79 explanatory variables describing various aspects of the homes. Your goal is to predict the sales price for each house, and the evaluation metric is the Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price.\n",
      "\n",
      "To proceed with your research, you can start by exploring the provided dataset files, such as 'data_description.txt', 'train.csv', and 'test.csv', to understand the features and the structure of the data. You may need to preprocess the data, handle missing values, and perform feature engineering to prepare it for model training.\n",
      "\n",
      "Next, you can choose and implement machine learning models to make predictions. It's important to split the training data into a training set and a validation set to evaluate the model's performance. You can use techniques such as cross-validation and hyperparameter tuning to optimize the model's performance.\n",
      "\n",
      "After training and validating the model, you can make predictions on the test set and create a submission file in the specified format. Ensure that the predictions meet the required criteria, such as a train and validation Mean Absolute Error (MAE) of lower than 15,000.\n",
      "\n",
      "If you encounter any challenges or uncertainties during the process, consider seeking help from the competition's community forums, exploring relevant resources, or consulting with experienced data scientists. Good luck with your research, and feel free to ask for further assistance as needed.\n",
      "\n",
      "Step: 2\n",
      "Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: Calling function wrapped_read_file(args = (), kwargs = {'max_char_read': 2000, 'file_name': 'data_description.txt', 'work_dir': '.'})\n",
      "Result: MSSubClass: Identifies the type of dwelling involved in the sale.\t\n",
      "\n",
      "        20\t1-STORY 1946 & NEWER ALL STYLES\n",
      "        30\t1-STORY 1945 & OLDER\n",
      "        40\t1-STORY W/FINISHED ATTIC ALL AGES\n",
      "        45\t1-1/2 STORY - UNFINISHED ALL AGES\n",
      "        50\t1-1/2 STORY FINISHED ALL AGES\n",
      "        60\t2-STORY 1946 & NEWER\n",
      "        70\t2-STORY 1945 & OLDER\n",
      "        75\t2-1/2 STORY ALL AGES\n",
      "        80\tSPLIT OR MULTI-LEVEL\n",
      "        85\tSPLIT FOYER\n",
      "        90\tDUPLEX - ALL STYLES AND AGES\n",
      "       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n",
      "       150\t1-1/2 STORY PUD - ALL AGES\n",
      "       160\t2-STORY PUD - 1946 & NEWER\n",
      "       180\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n",
      "       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES\n",
      "\n",
      "MSZoning: Identifies the general zoning classification of the sale.\n",
      "\t\t\n",
      "       A\tAgriculture\n",
      "       C\tCommercial\n",
      "       FV\tFloating Village Residential\n",
      "       I\tIndustrial\n",
      "       RH\tResidential High Density\n",
      "       RL\tResidential Low Density\n",
      "       RP\tResidential Low Density Park \n",
      "       RM\tResidential Medium Density\n",
      "\t\n",
      "LotFrontage: Linear feet of street connected to property\n",
      "\n",
      "LotArea: Lot size in square feet\n",
      "\n",
      "Street: Type of road access to property\n",
      "\n",
      "       Grvl\tGravel\t\n",
      "       Pave\tPaved\n",
      "       \t\n",
      "Alley: Type of alley access to property\n",
      "\n",
      "       Grvl\tGravel\n",
      "       Pave\tPaved\n",
      "       NA \tNo alley access\n",
      "\t\t\n",
      "LotShape: General shape of property\n",
      "\n",
      "       Reg\tRegular\t\n",
      "       IR1\tSlightly irregular\n",
      "       IR2\tModerately Irregular\n",
      "       IR3\tIrregular\n",
      "       \n",
      "LandContour: Flatness of the property\n",
      "\n",
      "       Lvl\tNear Flat/Level\t\n",
      "       Bnk\tBanked - Quick and significant rise from street grade to building\n",
      "       HLS\tHillside - Significant slope from side to side\n",
      "       Low\tDepression\n",
      "\t\t\n",
      "Utilities: Type of utilities available\n",
      "\t\t\n",
      "       AllPub\tAll public Utilities (E,G,W,& S)\t\n",
      "       NoSewr\tElectricity, Gas, and Water (Septic Tank)\n",
      "       NoSeWa\tElectricity and Gas Only\n",
      "       ELO\tElectricity only\t\n",
      "\t\n",
      "LotConfig: Lot configuration\n",
      "\n",
      "       Inside\tInside lot\n",
      "       Corner\tCorner lot\n",
      "       CulD\n",
      "Step: 3\n",
      "Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: Calling function wrapped_read_file(args = (), kwargs = {'max_char_read': 2000, 'file_name': 'train.csv', 'work_dir': '.'})\n",
      "Result: Id,MSSubClass,MSZoning,LotFrontage,LotArea,Street,Alley,LotShape,LandContour,Utilities,LotConfig,LandSlope,Neighborhood,Condition1,Condition2,BldgType,HouseStyle,OverallQual,OverallCond,YearBuilt,YearRemodAdd,RoofStyle,RoofMatl,Exterior1st,Exterior2nd,MasVnrType,MasVnrArea,ExterQual,ExterCond,Foundation,BsmtQual,BsmtCond,BsmtExposure,BsmtFinType1,BsmtFinSF1,BsmtFinType2,BsmtFinSF2,BsmtUnfSF,TotalBsmtSF,Heating,HeatingQC,CentralAir,Electrical,1stFlrSF,2ndFlrSF,LowQualFinSF,GrLivArea,BsmtFullBath,BsmtHalfBath,FullBath,HalfBath,BedroomAbvGr,KitchenAbvGr,KitchenQual,TotRmsAbvGrd,Functional,Fireplaces,FireplaceQu,GarageType,GarageYrBlt,GarageFinish,GarageCars,GarageArea,GarageQual,GarageCond,PavedDrive,WoodDeckSF,OpenPorchSF,EnclosedPorch,3SsnPorch,ScreenPorch,PoolArea,PoolQC,Fence,MiscFeature,MiscVal,MoSold,YrSold,SaleType,SaleCondition,SalePrice\n",
      "1,60,RL,65,8450,Pave,NA,Reg,Lvl,AllPub,Inside,Gtl,CollgCr,Norm,Norm,1Fam,2Story,7,5,2003,2003,Gable,CompShg,VinylSd,VinylSd,BrkFace,196,Gd,TA,PConc,Gd,TA,No,GLQ,706,Unf,0,150,856,GasA,Ex,Y,SBrkr,856,854,0,1710,1,0,2,1,3,1,Gd,8,Typ,0,NA,Attchd,2003,RFn,2,548,TA,TA,Y,0,61,0,0,0,0,NA,NA,NA,0,2,2008,WD,Normal,208500\n",
      "2,20,RL,80,9600,Pave,NA,Reg,Lvl,AllPub,FR2,Gtl,Veenker,Feedr,Norm,1Fam,1Story,6,8,1976,1976,Gable,CompShg,MetalSd,MetalSd,None,0,TA,TA,CBlock,Gd,TA,Gd,ALQ,978,Unf,0,284,1262,GasA,Ex,Y,SBrkr,1262,0,0,1262,0,1,2,0,3,1,TA,6,Typ,1,TA,Attchd,1976,RFn,2,460,TA,TA,Y,298,0,0,0,0,0,NA,NA,NA,0,5,2007,WD,Normal,181500\n",
      "3,60,RL,68,11250,Pave,NA,IR1,Lvl,AllPub,Inside,Gtl,CollgCr,Norm,Norm,1Fam,2Story,7,5,2001,2002,Gable,CompShg,VinylSd,VinylSd,BrkFace,162,Gd,TA,PConc,Gd,TA,Mn,GLQ,486,Unf,0,434,920,GasA,Ex,Y,SBrkr,920,866,0,1786,1,0,2,1,3,1,Gd,6,Typ,1,TA,Attchd,2001,RFn,2,608,TA,TA,Y,0,42,0,0,0,0,NA,NA,NA,0,9,2008,WD,Normal,223500\n",
      "4,70,RL,60,9550,Pave,NA,IR1,Lvl,AllPub,Corner,Gtl,Crawfor,Norm,Norm,1Fam,2Story,7,5,1915,1970,Gable,CompShg,Wd Sdng,Wd Shng,None,0,TA,TA,BrkTil,TA,Gd,No,ALQ,216,Unf,0,540,756,GasA,Gd,Y,SBrkr,961,756,0,1717,1\n",
      "Step: 4\n",
      "Files: ['data_description.txt', 'research_plan.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: Calling function wrapped_write_file(args = (), kwargs = {'file_name': 'research_plan.txt', 'content': \"Research Plan:\\n1. Explore the dataset files 'data_description.txt', 'train.csv', and 'test.csv' to understand the features and the structure of the data.\\n2. Preprocess the data, handle missing values, and perform feature engineering to prepare it for model training.\\n3. Choose and implement machine learning models to make predictions. Split the training data into a training set and a validation set to evaluate the model's performance.\\n4. Use techniques such as cross-validation and hyperparameter tuning to optimize the model's performance.\\n5. Make predictions on the test set and create a submission file in the specified format.\\n6. Ensure that the predictions meet the required criteria, such as a train and validation Mean Absolute Error (MAE) of lower than 15,000.\\n7. Seek help from the competition's community forums, explore relevant resources, or consult with experienced data scientists if encountering any challenges or uncertainties during the process.\\nGood luck with your research!\", 'work_dir': '.'})\n",
      "Result: File research_plan.txt written successfully.\n"
     ]
    }
   ],
   "source": [
    "execution_feedback = execution_agent.function_call(methods_prompt=methods_prompt)\n",
    "print(\"Execution feedback\", execution_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticAgent(Agent):\n",
    "    # TODO: Perhaps include the skills to the critic so the critic knows the facts to check if this makes sense or not\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def check_task_success(self, task, methods_prompt, execution_feedback):\n",
    "        system_prompt = '''You are an assistant that assesses my progress of research and provides useful guidance. \n",
    "        \n",
    "        You are required to evaluate if I have met the task requirements. Exceeding the task requirements is also considered a success while failing to meet them requires you to provide critique to help me improve .\n",
    "\n",
    "        I will give you the following information:\n",
    "        Skills: these are skills that I can take action with.\n",
    "        Files: these are my current files that I have in my working directory.\n",
    "        Task: The objective I need to accomplish.\n",
    "        History of files, action, and result (oldest to newest): By following the plan, this is my history of files, action, and result I had and took at that point in time.\n",
    "        Approach: My plan and reasoning to achieve the task.\n",
    "\n",
    "        You should only respond in JSON format as described below:\n",
    "        ```json\n",
    "        {\n",
    "            \"reasoning\": \"reasoning\",\n",
    "            \"success\": boolean,\n",
    "            \"critique\": \"critique\",\n",
    "        }\n",
    "        ```\n",
    "        Ensure the response can be parsed by Python \"json.loads\", e.g.: no trailing commas, no single quotes, etc.\n",
    "\n",
    "        Here are some examples:\n",
    "        INPUT:\n",
    "        Task: What is the distribution of the sale prices in the dataset?\n",
    "        Answer: To determine the distribution of the sale prices in the dataset, we can follow these steps:\\n\\n1. Read the dataset file \"train.csv\" using the `read_file` function.\\n2. Extract the column containing the sale prices from the dataset.\\n3. Calculate the frequency of each unique sale price in the dataset.\\n4. Sort the unique sale prices in ascending order.\\n5. Create a histogram or bar chart to visualize the distribution of the sale prices.\\n6. Optionally, you can also calculate summary statistics such as mean, median, and standard deviation of the sale prices.\\n\\nLet\\'s start by reading the dataset file \"train.csv\".\n",
    "        Approach: Task or question: What is the distribution of the sale prices in the dataset? \\nInstructions: To determine the distribution of the sale prices in the dataset, you can follow these steps:\\n\\n1. Read the dataset file \"train.csv\" using the `read_file` function.\\n2. Extract the column containing the sale prices from the dataset.\\n3. Calculate the frequency of each unique sale price in the dataset.\\n4. Sort the unique sale prices in ascending order.\\n5. Create a histogram or bar chart to visualize the distribution of the sale prices.\\n6. Optionally, you can also calculate summary statistics such as mean, median, and standard deviation of the sale prices.\\n\\nPlease note that the specific implementation details may vary depending on the programming language and libraries you are using.\n",
    "\n",
    "        RESPONSE:\n",
    "        {\n",
    "            \"reasoning\": \"The reasoning to get to the answer makes sense, but there's no direct answer for what the actual distribution of the sale price is.\",\n",
    "            \"success\": False,\n",
    "            \"critique\": \"The answer only tells us how to get the distribution is, but does not tell us what the actual distribution. Please tell us what the actual distribution is.\",\n",
    "        }\n",
    "        '''\n",
    "\n",
    "        user_prompt = f'''Skills: {list(self.available_actions.keys())}\n",
    "        Files: {self.files}\n",
    "        Task: {task}\n",
    "        History of files, action, and result: {execution_feedback}\n",
    "        Approach: {methods_prompt}'''\n",
    "\n",
    "        response_message = complete_text_openai(system_prompt=system_prompt, prompt=user_prompt, json_required=True) # TODO: probably allow the critic agent to just read files\n",
    "\n",
    "        try:\n",
    "            response_json = json.loads(response_message)\n",
    "            success = response_json['success']\n",
    "            reasoning = response_json['reasoning']\n",
    "            critique = response_json['critique']\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            return False, response_message + \" JSON parsing error: \" + str(e)\n",
    "\n",
    "        print(\"System prompt: \", system_prompt, \"\\n\\nUser prompt: \", user_prompt, \"\\n\\nResponse: \", response_message)\n",
    "        return success, reasoning + critique\n",
    "critic_agent = CriticAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In complete_text_openai(), Completion JSON:  {'reasoning': \"You have successfully read the 'data_description.txt' file to understand the meaning of each variable in the dataset. You have also received guidance on how to proceed with your research. Additionally, you have documented a research plan in the 'research_plan.txt' file.\", 'success': True, 'critique': ''}\n",
      "System prompt:  You are an assistant that assesses my progress of research and provides useful guidance. \n",
      "        \n",
      "        You are required to evaluate if I have met the task requirements. Exceeding the task requirements is also considered a success while failing to meet them requires you to provide critique to help me improve .\n",
      "\n",
      "        I will give you the following information:\n",
      "        Skills: these are skills that I can take action with.\n",
      "        Files: these are my current files that I have in my working directory.\n",
      "        Task: The objective I need to accomplish.\n",
      "        History of files, action, and result (oldest to newest): By following the plan, this is my history of files, action, and result I had and took at that point in time.\n",
      "        Approach: My plan and reasoning to achieve the task.\n",
      "\n",
      "        You should only respond in JSON format as described below:\n",
      "        ```json\n",
      "        {\n",
      "            \"reasoning\": \"reasoning\",\n",
      "            \"success\": boolean,\n",
      "            \"critique\": \"critique\",\n",
      "        }\n",
      "        ```\n",
      "        Ensure the response can be parsed by Python \"json.loads\", e.g.: no trailing commas, no single quotes, etc.\n",
      "\n",
      "        Here are some examples:\n",
      "        INPUT:\n",
      "        Task: What is the distribution of the sale prices in the dataset?\n",
      "        Answer: To determine the distribution of the sale prices in the dataset, we can follow these steps:\n",
      "\n",
      "1. Read the dataset file \"train.csv\" using the `read_file` function.\n",
      "2. Extract the column containing the sale prices from the dataset.\n",
      "3. Calculate the frequency of each unique sale price in the dataset.\n",
      "4. Sort the unique sale prices in ascending order.\n",
      "5. Create a histogram or bar chart to visualize the distribution of the sale prices.\n",
      "6. Optionally, you can also calculate summary statistics such as mean, median, and standard deviation of the sale prices.\n",
      "\n",
      "Let's start by reading the dataset file \"train.csv\".\n",
      "        Approach: Task or question: What is the distribution of the sale prices in the dataset? \n",
      "Instructions: To determine the distribution of the sale prices in the dataset, you can follow these steps:\n",
      "\n",
      "1. Read the dataset file \"train.csv\" using the `read_file` function.\n",
      "2. Extract the column containing the sale prices from the dataset.\n",
      "3. Calculate the frequency of each unique sale price in the dataset.\n",
      "4. Sort the unique sale prices in ascending order.\n",
      "5. Create a histogram or bar chart to visualize the distribution of the sale prices.\n",
      "6. Optionally, you can also calculate summary statistics such as mean, median, and standard deviation of the sale prices.\n",
      "\n",
      "Please note that the specific implementation details may vary depending on the programming language and libraries you are using.\n",
      "\n",
      "        RESPONSE:\n",
      "        {\n",
      "            \"reasoning\": \"The reasoning to get to the answer makes sense, but there's no direct answer for what the actual distribution of the sale price is.\",\n",
      "            \"success\": False,\n",
      "            \"critique\": \"The answer only tells us how to get the distribution is, but does not tell us what the actual distribution. Please tell us what the actual distribution is.\",\n",
      "        }\n",
      "         \n",
      "\n",
      "User prompt:  Skills: ['reflection', 'readFile', 'writeFile', 'executeScript']\n",
      "        Files: ['data_description.txt', 'research_plan.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "        Task: Read 'data_description.txt' to understand the meaning of each variable in the dataset.\n",
      "        History of files, action, and result: \n",
      "Step: 0\n",
      "Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: None\n",
      "Result: None\n",
      "Step: 1\n",
      "Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: Calling function wrapped_reflection(args = (), kwargs = {'things_to_reflect_on': \"I'm not sure what action to take next. I need more information to proceed.\", 'work_dir': '.'})\n",
      "Result: Reflection: It seems like you are participating in a data science competition to predict the sales price of residential homes in Ames, Iowa. The dataset contains 79 explanatory variables describing various aspects of the homes. Your goal is to predict the sales price for each house, and the evaluation metric is the Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price.\n",
      "\n",
      "To proceed with your research, you can start by exploring the provided dataset files, such as 'data_description.txt', 'train.csv', and 'test.csv', to understand the features and the structure of the data. You may need to preprocess the data, handle missing values, and perform feature engineering to prepare it for model training.\n",
      "\n",
      "Next, you can choose and implement machine learning models to make predictions. It's important to split the training data into a training set and a validation set to evaluate the model's performance. You can use techniques such as cross-validation and hyperparameter tuning to optimize the model's performance.\n",
      "\n",
      "After training and validating the model, you can make predictions on the test set and create a submission file in the specified format. Ensure that the predictions meet the required criteria, such as a train and validation Mean Absolute Error (MAE) of lower than 15,000.\n",
      "\n",
      "If you encounter any challenges or uncertainties during the process, consider seeking help from the competition's community forums, exploring relevant resources, or consulting with experienced data scientists. Good luck with your research, and feel free to ask for further assistance as needed.\n",
      "\n",
      "Step: 2\n",
      "Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: Calling function wrapped_read_file(args = (), kwargs = {'max_char_read': 2000, 'file_name': 'data_description.txt', 'work_dir': '.'})\n",
      "Result: MSSubClass: Identifies the type of dwelling involved in the sale.\t\n",
      "\n",
      "        20\t1-STORY 1946 & NEWER ALL STYLES\n",
      "        30\t1-STORY 1945 & OLDER\n",
      "        40\t1-STORY W/FINISHED ATTIC ALL AGES\n",
      "        45\t1-1/2 STORY - UNFINISHED ALL AGES\n",
      "        50\t1-1/2 STORY FINISHED ALL AGES\n",
      "        60\t2-STORY 1946 & NEWER\n",
      "        70\t2-STORY 1945 & OLDER\n",
      "        75\t2-1/2 STORY ALL AGES\n",
      "        80\tSPLIT OR MULTI-LEVEL\n",
      "        85\tSPLIT FOYER\n",
      "        90\tDUPLEX - ALL STYLES AND AGES\n",
      "       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n",
      "       150\t1-1/2 STORY PUD - ALL AGES\n",
      "       160\t2-STORY PUD - 1946 & NEWER\n",
      "       180\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n",
      "       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES\n",
      "\n",
      "MSZoning: Identifies the general zoning classification of the sale.\n",
      "\t\t\n",
      "       A\tAgriculture\n",
      "       C\tCommercial\n",
      "       FV\tFloating Village Residential\n",
      "       I\tIndustrial\n",
      "       RH\tResidential High Density\n",
      "       RL\tResidential Low Density\n",
      "       RP\tResidential Low Density Park \n",
      "       RM\tResidential Medium Density\n",
      "\t\n",
      "LotFrontage: Linear feet of street connected to property\n",
      "\n",
      "LotArea: Lot size in square feet\n",
      "\n",
      "Street: Type of road access to property\n",
      "\n",
      "       Grvl\tGravel\t\n",
      "       Pave\tPaved\n",
      "       \t\n",
      "Alley: Type of alley access to property\n",
      "\n",
      "       Grvl\tGravel\n",
      "       Pave\tPaved\n",
      "       NA \tNo alley access\n",
      "\t\t\n",
      "LotShape: General shape of property\n",
      "\n",
      "       Reg\tRegular\t\n",
      "       IR1\tSlightly irregular\n",
      "       IR2\tModerately Irregular\n",
      "       IR3\tIrregular\n",
      "       \n",
      "LandContour: Flatness of the property\n",
      "\n",
      "       Lvl\tNear Flat/Level\t\n",
      "       Bnk\tBanked - Quick and significant rise from street grade to building\n",
      "       HLS\tHillside - Significant slope from side to side\n",
      "       Low\tDepression\n",
      "\t\t\n",
      "Utilities: Type of utilities available\n",
      "\t\t\n",
      "       AllPub\tAll public Utilities (E,G,W,& S)\t\n",
      "       NoSewr\tElectricity, Gas, and Water (Septic Tank)\n",
      "       NoSeWa\tElectricity and Gas Only\n",
      "       ELO\tElectricity only\t\n",
      "\t\n",
      "LotConfig: Lot configuration\n",
      "\n",
      "       Inside\tInside lot\n",
      "       Corner\tCorner lot\n",
      "       CulD\n",
      "Step: 3\n",
      "Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: Calling function wrapped_read_file(args = (), kwargs = {'max_char_read': 2000, 'file_name': 'train.csv', 'work_dir': '.'})\n",
      "Result: Id,MSSubClass,MSZoning,LotFrontage,LotArea,Street,Alley,LotShape,LandContour,Utilities,LotConfig,LandSlope,Neighborhood,Condition1,Condition2,BldgType,HouseStyle,OverallQual,OverallCond,YearBuilt,YearRemodAdd,RoofStyle,RoofMatl,Exterior1st,Exterior2nd,MasVnrType,MasVnrArea,ExterQual,ExterCond,Foundation,BsmtQual,BsmtCond,BsmtExposure,BsmtFinType1,BsmtFinSF1,BsmtFinType2,BsmtFinSF2,BsmtUnfSF,TotalBsmtSF,Heating,HeatingQC,CentralAir,Electrical,1stFlrSF,2ndFlrSF,LowQualFinSF,GrLivArea,BsmtFullBath,BsmtHalfBath,FullBath,HalfBath,BedroomAbvGr,KitchenAbvGr,KitchenQual,TotRmsAbvGrd,Functional,Fireplaces,FireplaceQu,GarageType,GarageYrBlt,GarageFinish,GarageCars,GarageArea,GarageQual,GarageCond,PavedDrive,WoodDeckSF,OpenPorchSF,EnclosedPorch,3SsnPorch,ScreenPorch,PoolArea,PoolQC,Fence,MiscFeature,MiscVal,MoSold,YrSold,SaleType,SaleCondition,SalePrice\n",
      "1,60,RL,65,8450,Pave,NA,Reg,Lvl,AllPub,Inside,Gtl,CollgCr,Norm,Norm,1Fam,2Story,7,5,2003,2003,Gable,CompShg,VinylSd,VinylSd,BrkFace,196,Gd,TA,PConc,Gd,TA,No,GLQ,706,Unf,0,150,856,GasA,Ex,Y,SBrkr,856,854,0,1710,1,0,2,1,3,1,Gd,8,Typ,0,NA,Attchd,2003,RFn,2,548,TA,TA,Y,0,61,0,0,0,0,NA,NA,NA,0,2,2008,WD,Normal,208500\n",
      "2,20,RL,80,9600,Pave,NA,Reg,Lvl,AllPub,FR2,Gtl,Veenker,Feedr,Norm,1Fam,1Story,6,8,1976,1976,Gable,CompShg,MetalSd,MetalSd,None,0,TA,TA,CBlock,Gd,TA,Gd,ALQ,978,Unf,0,284,1262,GasA,Ex,Y,SBrkr,1262,0,0,1262,0,1,2,0,3,1,TA,6,Typ,1,TA,Attchd,1976,RFn,2,460,TA,TA,Y,298,0,0,0,0,0,NA,NA,NA,0,5,2007,WD,Normal,181500\n",
      "3,60,RL,68,11250,Pave,NA,IR1,Lvl,AllPub,Inside,Gtl,CollgCr,Norm,Norm,1Fam,2Story,7,5,2001,2002,Gable,CompShg,VinylSd,VinylSd,BrkFace,162,Gd,TA,PConc,Gd,TA,Mn,GLQ,486,Unf,0,434,920,GasA,Ex,Y,SBrkr,920,866,0,1786,1,0,2,1,3,1,Gd,6,Typ,1,TA,Attchd,2001,RFn,2,608,TA,TA,Y,0,42,0,0,0,0,NA,NA,NA,0,9,2008,WD,Normal,223500\n",
      "4,70,RL,60,9550,Pave,NA,IR1,Lvl,AllPub,Corner,Gtl,Crawfor,Norm,Norm,1Fam,2Story,7,5,1915,1970,Gable,CompShg,Wd Sdng,Wd Shng,None,0,TA,TA,BrkTil,TA,Gd,No,ALQ,216,Unf,0,540,756,GasA,Gd,Y,SBrkr,961,756,0,1717,1\n",
      "Step: 4\n",
      "Files: ['data_description.txt', 'research_plan.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: Calling function wrapped_write_file(args = (), kwargs = {'file_name': 'research_plan.txt', 'content': \"Research Plan:\\n1. Explore the dataset files 'data_description.txt', 'train.csv', and 'test.csv' to understand the features and the structure of the data.\\n2. Preprocess the data, handle missing values, and perform feature engineering to prepare it for model training.\\n3. Choose and implement machine learning models to make predictions. Split the training data into a training set and a validation set to evaluate the model's performance.\\n4. Use techniques such as cross-validation and hyperparameter tuning to optimize the model's performance.\\n5. Make predictions on the test set and create a submission file in the specified format.\\n6. Ensure that the predictions meet the required criteria, such as a train and validation Mean Absolute Error (MAE) of lower than 15,000.\\n7. Seek help from the competition's community forums, explore relevant resources, or consult with experienced data scientists if encountering any challenges or uncertainties during the process.\\nGood luck with your research!\", 'work_dir': '.'})\n",
      "Result: File research_plan.txt written successfully.\n",
      "        Approach: None \n",
      "\n",
      "Response:  {\n",
      "    \"reasoning\": \"You have successfully read the 'data_description.txt' file to understand the meaning of each variable in the dataset. You have also received guidance on how to proceed with your research. Additionally, you have documented a research plan in the 'research_plan.txt' file.\",\n",
      "    \"success\": true,\n",
      "    \"critique\": \"\"\n",
      "}\n",
      "Success:  True \n",
      "Critique:  You have successfully read the 'data_description.txt' file to understand the meaning of each variable in the dataset. You have also received guidance on how to proceed with your research. Additionally, you have documented a research plan in the 'research_plan.txt' file.\n"
     ]
    }
   ],
   "source": [
    "success, critique = critic_agent.check_task_success(task=next_task, methods_prompt=methods_prompt, execution_feedback=execution_feedback)\n",
    "print(\"Success: \", success, \"\\nCritique: \", critique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate only once! \n",
    "skill_manager = SkillManager(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # START HERE FOR NEXT ITERATIONS: Save the current iteration's state after this cycle in case we need to revert\n",
    "# skill_manager_copy = copy.deepcopy(skill_manager)\n",
    "# curriculum_agent_copy = copy.deepcopy(curriculum_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # REVERT BY STARTING HERE, uncomment the below and run\n",
    "# skill_manager = copy.deepcopy(skill_manager_copy)\n",
    "# curriculum_agent = copy.deepcopy(curriculum_agent_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploration progress:  Completed tasks: [], Failed tasks: []\n"
     ]
    }
   ],
   "source": [
    "exploration_progress = curriculum_agent.get_exploration_progress()\n",
    "print(\"Exploration progress: \", exploration_progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt for generating curriculum: \n",
      " You are a helpful assistant that tells me the next immediate task to do. My ultimate goal is to discover as many useful pieces of information as possible to better achieve the research goal, answer as many questions as possible to get the best answer, and become the best researcher in the world in solving this research goal.\n",
      "\n",
      "        Research Goal: Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n",
      "\n",
      "With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n",
      "\n",
      "Evaluation\n",
      "Goal\n",
      "It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. You want a train and validation MAE of lower than 15,000 and there should be a submission.csv containing predictions for test.csv ready to submit.\n",
      "\n",
      "Metric\n",
      "Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n",
      "\n",
      "Submission File Format\n",
      "The file should contain a header and have the following format:\n",
      "\n",
      "Id,SalePrice\n",
      "1461,169000.1\n",
      "1462,187724.1233\n",
      "1463,175221\n",
      "etc.\n",
      "\n",
      "        I will give you the following information:\n",
      "        Skills: these are skills that I can take action with.\n",
      "        Files: these are my current files that I have in my working directory.\n",
      "        Most recent files, action, result, and answer states (oldest to newest): Answer states are the a report of the best answer I have so far to achieving the research goal, and the files, action, and result are the files, action, and result I took at that point in time to produce the answer state.    \n",
      "        Completed tasks so far: ...\n",
      "        Failed tasks that are too hard: ...\n",
      "\n",
      "        1) You should act as a mentor and guide me to the next task based on my current learning progress.\n",
      "        2) Please be very specific about what information or actions I need to take and what expected results I need to achieve.\n",
      "        3) The next task should follow a clear format, such as \"Write [file]\", \"Reflect on why I'm seeing [error]\", \"Read [file]\", \"Brainstorm if ideas from [topic 1] be applied to [topic 2]\", \"Analyze what are the similarities between [topic 1] for success and [topic 2]\" , \"Reflect on what's significant about this paper: [paper]?\", \"Reflect on what's currently missing or preventing me from achieving [goal] better\", etc. It should be a single task to collect useful information on. Do not propose multiple tasks at the same time. Do not mention anything else. \n",
      "        4) The next task should not be too hard since the internet and I may not contain the full answer in a single article or have learned enough information to complete it yet. \n",
      "        5) The next task should be novel and interesting based on my current learning progress. I should look for rare and potentially useful pieces of information, upgrade my current answer using better information, and discover new things. I should not be doing the same thing over and over again.\n",
      "        6) I may sometimes need to repeat some tasks or variations of the task if I need to collect more information to answer more difficult tasks. Only repeat tasks if necessary. \n",
      "        7) I want to explore the world and discover new things. I don’t want to stay with my current answer for too long. \n",
      "        8) Tasks that require information beyond another reader's ability to theoretically verify and reason if completed or correct should be avoided. For instance, \"what else is there on the website?\" and \"what images and tables are on the website\" are not ideal since they require visual confirmation from the screen. All the testing, coding, and asking other people questions should be avoided. Do not propose a task  with these keywords. You should only respond in the format as described below:\n",
      "        \n",
      "        RESPONSE FORMAT: \n",
      "        ```json\n",
      "        { \n",
      "            \"reasoning\": \"<based on the information I listed above, do reasoning about what the next task should be.>\",\n",
      "            \"task\": \"<the next task.>\"\n",
      "        }\n",
      "        ```\n",
      "        \n",
      "        Here’s an example response: \n",
      "        ```json\n",
      "        { \n",
      "            \"reasoning\": \"We know that we have a sword and we know there's fire, and fire lights things on fire. Therefore, we could try to make a firesword.\",\n",
      "            \"task\": \"Try to make a firesword and record what happens.\"\n",
      "        }\n",
      "        ```\n",
      "\n",
      "        Ensure the response can be parsed by Python \"json.loads\", e.g.: no trailing commas, no single quotes, etc. This is important.\n",
      " \n",
      " User prompt:  Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Skills: ['reflection', 'readFile', 'writeFile', 'executeScript']\n",
      "Most recent files, action, result, and answer states (oldest to newest):\n",
      "\n",
      "Step: 0\n",
      "Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n",
      "Action: None\n",
      "Result: None\n",
      "Answer: None      \n",
      "Completed tasks so far: []\n",
      "Failed tasks that are too hard: []\n",
      "\n",
      "In complete_text_openai(), Completion JSON:  {'reasoning': 'Since you have the dataset files and the data description, it would be beneficial to start by reading the data description to understand the meaning and structure of the variables in the dataset. This will provide valuable insights into the features that may influence the sale price of the houses.', 'task': \"Read 'data_description.txt' to understand the variables and their meanings in the dataset.\"}\n",
      "Response:  {\n",
      "  \"reasoning\": \"Since you have the dataset files and the data description, it would be beneficial to start by reading the data description to understand the meaning and structure of the variables in the dataset. This will provide valuable insights into the features that may influence the sale price of the houses.\",\n",
      "  \"task\": \"Read 'data_description.txt' to understand the variables and their meanings in the dataset.\"\n",
      "}\n",
      "next_task Read 'data_description.txt' to understand the variables and their meanings in the dataset.\n"
     ]
    }
   ],
   "source": [
    "next_task = curriculum_agent.propose_next_task()\n",
    "print(\"next_task\", next_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_task Read 'data_description.txt' to understand the variables and their meanings in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Attempting to build 1 cycle with all agents\n",
    "# next_question = \"Search up how many penguins there are in Kevin Huang's house. We don't need an exact or true number.\"\n",
    "# next_question = \"What is the total number of penguins and children that we know about from our functions?\" # hardcoded for now\n",
    "num_rounds = 2\n",
    "num_tasks = 1\n",
    "print(\"next_task\", next_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 0 Task: \n",
      " Read 'data_description.txt' to understand the variables and their meanings in the dataset.\n",
      "\n",
      "Skill manager output:\n",
      "Skills:  ['reflection', 'readFile', 'writeFile', 'executeScript']\n",
      "\n",
      "Starting methods agent\n",
      "methods_agent_feedback Plan: \n",
      "To understand the variables and their meanings in the dataset, we will read the 'data_description.txt' file.\n",
      "\n",
      "Steps:\n",
      "1. Read the content of the 'data_description.txt' file to understand the variables and their meanings in the dataset.\n",
      "2. Use the information obtained from the 'data_description.txt' file to gain insights into the features and structure of the dataset.\n",
      "\n",
      "Methods agent output:\n",
      " Task: Read 'data_description.txt' to understand the variables and their meanings in the dataset.\n",
      "Instructions: Plan: \n",
      "To understand the variables and their meanings in the dataset, we will read the 'data_description.txt' file.\n",
      "\n",
      "Steps:\n",
      "1. Read the content of the 'data_description.txt' file to understand the variables and their meanings in the dataset.\n",
      "2. Use the information obtained from the 'data_description.txt' file to gain insights into the features and structure of the dataset.\n",
      "\n",
      "Starting execution agent\n",
      "\n",
      "run.status:  queued\n",
      "\n",
      "run.status:  requires_action\n",
      "Run required action: \n",
      "tool_id: call_Ma8l33xo0TUFFWp667n1hCts, \n",
      "tool_function.arguments: {\"file_name\":\"data_description.txt\"}, \n",
      "tool_function.name: readFile, \n",
      "tool_type: function\n",
      "\n",
      "Step: 5\n",
      "Calling function wrapped_read_file(args = (), kwargs = {'max_char_read': 2000, 'file_name': 'data_description.txt', 'work_dir': 'workspace\\\\home-data-for-ml-course_branch'})\n",
      "\n",
      "--- TOOL SUCCESS ---\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "for task_idx in range(num_tasks):\n",
    "    methods_prompt = None\n",
    "    execution_feedback = None\n",
    "    execution_errors = None\n",
    "    critique = None\n",
    "    success = False\n",
    "    for i in range (num_rounds):\n",
    "        print(f\"\\nRound {i} Task: \\n\", next_task)\n",
    "        \n",
    "        skills = skill_manager.retrieve_skills(next_task, execution_feedback)\n",
    "        print(\"\\nSkill manager output:\\nSkills: \", skills)\n",
    "\n",
    "        print(\"\\nStarting methods agent\")\n",
    "        methods_agent = MethodsAgent(env)\n",
    "        methods_prompt = methods_agent.generate_function_callable_prompt(task=next_task, methods_prompt=methods_prompt, execution_feedback=execution_feedback, execution_errors=execution_errors, critique=critique, skills=list(skill_manager.available_actions.keys()))\n",
    "        print(\"\\nMethods agent output:\\n\", methods_prompt)\n",
    "\n",
    "        print(\"\\nStarting execution agent\")\n",
    "        execution_agent = ExecutionAgent(env)\n",
    "        execution_feedback = execution_agent.function_call(methods_prompt=methods_prompt)\n",
    "        print(\"\\nExecution agent output: \", execution_feedback)\n",
    "\n",
    "        print(\"\\nStarting critic agent\")\n",
    "        critic_agent = CriticAgent(env)\n",
    "        success, critique = critic_agent.check_task_success(task=next_task, methods_prompt=methods_prompt, execution_feedback=execution_feedback)\n",
    "        print(\"Critic agent output\", \"\\nSuccess: \", success, \"\\nCritique: \", critique)\n",
    "\n",
    "        if success:\n",
    "            break\n",
    "    if success:\n",
    "        # print(\"\\nBefore adding info block\\n\", skill_manager.retrieve_info_blocks(next_task, execution_feedback=None))\n",
    "        # skill_manager.add_skill(next_task, execution_feedback, methods_prompt)\n",
    "        # print(\"\\nAfter adding info block\\n\", skill_manager.retrieve_info_blocks(next_task, execution_feedback=None))\n",
    "        curriculum_agent.add_completed_task(\"Task: \" + next_task + \" \\nAnswer: \" + execution_feedback)\n",
    "    else:\n",
    "        curriculum_agent.add_failed_task(\"Task: \" + next_task + \" \\nBest answer: \" + execution_feedback + \" \\nCritique for why it failed: \" + critique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
