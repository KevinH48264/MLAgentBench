{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New working directory:  c:\\Users\\kevihuang\\OneDrive - Microsoft\\Desktop\\projects\\MLAgentBench\\MLAgentBench_v2\\agents\\Voyager\\..\\..\\..\n",
      "New Working Directory: c:\\Users\\kevihuang\\OneDrive - Microsoft\\Desktop\\projects\\MLAgentBench\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the new working directory relative to the current working directory\n",
    "# or use an absolute path\n",
    "new_working_directory = os.path.join(os.getcwd(), '..', '..', '..') # Set to MLAgentBenhc\n",
    "print(\"New working directory: \", new_working_directory)\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(new_working_directory)\n",
    "\n",
    "print(\"New Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLAgentBench_v2.agents.agent import Agent\n",
    "import numpy as np \n",
    "import json\n",
    "import logging \n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time \n",
    "import types\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'helm'\n",
      "Could not load CRFM API key crfm_api_key.txt.\n",
      "[Errno 2] No such file or directory: 'claude_api_key.txt'\n",
      "Could not load anthropic API key claude_api_key.txt.\n",
      "Initializing environment...\n",
      "args namespace(task='home-data-for-ml-course', task_type='kaggle', log_dir='logs/house-price-testing_voyager-v2_gpt4_v4_questioning_wiki', work_dir='workspace', max_steps=50, max_time=18000, device=0, python='/home/user/micromamba/envs/autogpt/bin/python', interactive=False, resume=None, resume_step=0, agent_type='SimpleAssistantAgent', llm_name='gpt-4-1106-preview', fast_llm_name='gpt-4-1106-preview', edit_script_llm_name='gpt-4-1106-preview', edit_script_llm_max_tokens=4000, agent_max_steps=50, actions_remove_from_prompt=[], actions_add_to_prompt=[], no_retrieval=False, valid_format_entires=None, max_steps_in_context=3, max_observation_steps_in_context=3, max_retries=4, langchain_agent='zero-shot-react-description')\n",
      "Preparing task home-data-for-ml-course , of type:  kaggle\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an environment\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from MLAgentBench_v2.environment import Environment\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    task='home-data-for-ml-course',\n",
    "    task_type='kaggle',\n",
    "    log_dir='logs/house-price-testing_voyager-v2_gpt4_v4_questioning_wiki',\n",
    "    work_dir='workspace',\n",
    "    max_steps=50,\n",
    "    max_time=18000,\n",
    "    device=0,\n",
    "    python='/home/user/micromamba/envs/autogpt/bin/python',\n",
    "    interactive=False,\n",
    "    resume=None,\n",
    "    resume_step=0,\n",
    "    agent_type='SimpleAssistantAgent',\n",
    "    # llm_name='gpt-3.5-turbo-1106',\n",
    "    # fast_llm_name='gpt-3.5-turbo-1106',\n",
    "    # edit_script_llm_name='gpt-3.5-turbo-1106',\n",
    "    llm_name='gpt-4-1106-preview',\n",
    "    fast_llm_name='gpt-4-1106-preview',\n",
    "    edit_script_llm_name='gpt-4-1106-preview',\n",
    "    edit_script_llm_max_tokens=4000,\n",
    "    agent_max_steps=50,\n",
    "    actions_remove_from_prompt=[],\n",
    "    actions_add_to_prompt=[],\n",
    "    no_retrieval=False,\n",
    "    valid_format_entires=None,\n",
    "    max_steps_in_context=3,\n",
    "    max_observation_steps_in_context=3,\n",
    "    max_retries=4,\n",
    "    langchain_agent='zero-shot-react-description'\n",
    ")\n",
    "\n",
    "env = Environment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurriculumAgent(Agent):\n",
    "    def __init__(self, env, completed_tasks=[], failed_tasks=[]):\n",
    "        super().__init__(env)\n",
    "        self.completed_tasks = completed_tasks\n",
    "        self.failed_tasks = failed_tasks\n",
    "\n",
    "        # The crux is a Q&A process\n",
    "        # Problem with this approach is you still have to deal with searching multiple times, and continuing to search or not. Approach: Or maybe if you search and you don't have the answer, that's a bad thing to search and you need to go more specific / ask a different question!\n",
    "        self.system_prompt_automatic_curriculum = f'''You are a helpful assistant that tells me the next immediate task to do. My ultimate goal is to discover as many useful pieces of information as possible to better achieve the research goal, answer as many questions as possible to get the best answer, and become the best researcher in the world in solving this research goal.\n",
    "\n",
    "Research Goal: {self.research_problem}\n",
    "\n",
    "I will give you the following information:\n",
    "Question 1: ...\n",
    "Answer: ...\n",
    "Question 2: ...\n",
    "Answer: ...\n",
    "Question 3: ...\n",
    "Answer: ...\n",
    "...\n",
    "Files: these are my current files and skills that I have in my working directory.\n",
    "Skills: these are skills that I can take action with.\n",
    "Completed tasks so far (most recent to least): ...\n",
    "Failed tasks that are too hard (most recent to least): ...\n",
    "Most recent attempted tasks, plans, results, files, and answer states (newest to oldest): Answer states are the report of the best answer I have so far to achieving the research goal, and the attempted tasks, plans, results, and files are the tasks, plans, results, and files I took and had at that point in time to update the answer state.\n",
    "\n",
    "1) You should act as a mentor and guide me to the next task based on my current learning progress. Always give me the task that will help me learn the most and reach my research goal the fastest. \n",
    "2) Please be very specific about what information or actions I need to take and what expected results I need to achieve.\n",
    "3) The next task should follow a clear format, such as \"Write [file]\", \"Reflect on why I'm seeing [error]\", \"Read [file]\", \"Brainstorm if ideas from [topic 1] be applied to [topic 2]\", \"Analyze what are the similarities between [topic 1] for success and [topic 2]\" , \"Reflect on what's significant about this paper: [paper]?\", \"Reflect on what's currently missing or preventing me from achieving [goal] better\", etc. It should be a single task to collect useful information on. Do not propose multiple tasks at the same time. Do not mention anything else. \n",
    "4) The next task should not be too hard since the internet and I may not contain the full answer in a single article or have learned enough information to complete it yet. \n",
    "5) The next task should be novel and interesting based on my current learning progress. I should look for rare and potentially useful pieces of information, upgrade my current answer using better information, and discover new things. I should not be doing the same thing over and over again.\n",
    "6) I may sometimes need to repeat some tasks or variations of the task if I need to collect more information to answer more difficult tasks. Only repeat tasks if necessary. \n",
    "7) I want to explore the world and discover new things. I don’t want to stay with my current answer for too long. \n",
    "8) Tasks that require information beyond another reader's ability to theoretically verify and reason if completed or correct should be avoided. For instance, \"what else is there on the website?\" and \"what images and tables are on the website\" are not ideal since they require visual confirmation from the screen. All the testing, coding, and asking other people questions should be avoided. Do not propose a task  with these keywords. You should only respond in the format as described below:\n",
    "\n",
    "RESPONSE FORMAT: \n",
    "```json\n",
    "{{ \n",
    "    \"reasoning\": \"<based on the information I listed above, do reasoning about what the next task should be.>\",\n",
    "    \"task\": \"<the next task.>\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Here’s an example response: \n",
    "```json\n",
    "{{ \n",
    "    \"reasoning\": \"We know that we have a sword and we know there's fire, and fire lights things on fire. Therefore, we could try to make a firesword.\",\n",
    "    \"task\": \"Try to make a firesword and record what happens.\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Ensure the response can be parsed by Python \"json.loads\", e.g.: no trailing commas, no single quotes, etc. This is important.\n",
    "'''\n",
    "\n",
    "        # TODO: This is optional, might be useful, but to focus on a system prompt of asking questions and answering questions.\n",
    "        # System 2: this is a more scoped down version where we have the focus be on only answering questions -- reading and analyzing information & asking questions. No action items. \n",
    "        # The current above system 1 is better for self-driving labs type of work where there are going to be more tasks.\n",
    "\n",
    "    def get_exploration_progress(self):\n",
    "        # TODO: this should contain inventory of where we're at now and what files we have / memory stream\n",
    "        return f'''Completed tasks: {self.completed_tasks}, Failed tasks: {self.failed_tasks}'''\n",
    "\n",
    "    def retrieve_from_wiki(self):\n",
    "        # This function should solicit 5 questions from the agent and retrieve information from Wikipedia pages about it\n",
    "        # The answer should be returned as a string in Q: ... A: ... ... format\n",
    "\n",
    "        # First ask for questions\n",
    "        asking_questions_system_prompt = f'''You are a helpful assistant that asks questions to help me decide the next immediate task to do in research. My ultimate goal is to discover as many useful pieces of information as possible to better achieve the research goal, answer as many questions as possible to get the best answer, and become the best researcher in the world in solving this research goal.\n",
    "\n",
    "Research Goal: {self.research_problem}\n",
    "\n",
    "I will give you the following information:\n",
    "Files: these are my current files and skills that I have in my working directory.\n",
    "Skills: these are skills that I can take action with.\n",
    "Completed tasks so far (most recent to least): ...\n",
    "Failed tasks that are too hard (most recent to least): ...\n",
    "Most recent attempted tasks, plans, results, files, and answer states (newest to oldest): Answer states are the report of the best answer I have so far to achieving the research goal, and the attempted tasks, plans, results, and files are the tasks, plans, results, and files I took and had at that point in time to update the answer state.\n",
    "\n",
    "You must follow the following critiera:\n",
    "1) You should ask at least 5 questions (but no more than 10 questions) to help me decide the next immediate task to do. Each quesiton should be followed by the concept that the question is about.\n",
    "2) You question should be specific to a concept in Wikipedia. The question should not be too general.\n",
    "Bad example (the question is too general):\n",
    "Question: What is the best way to achieve the research goal?\n",
    "Concept: unknown\n",
    "Good example:\n",
    "Question: What are some predictive models that can be used to predict the SalePrice of a house?\n",
    "Concept: housing price predictive model\n",
    "3) Your questions should be self-contained and not require any context.\n",
    "Bad example (the question requires the context of my current files):\n",
    "Question: Have you checked 'submission.csv' to ensure that the predicted SalePrice values are in a reasonable range compared to the distribution of SalePrice in 'train.csv'?\n",
    "Concept: unknown\n",
    "Bad example (the question requires the context of my current files):\n",
    "Question: Does the 'model_training_script.py' include a cross-validation process to ensure the model's performance is robust and not overfitting?\n",
    "Concept: unknown\n",
    "Good example: \n",
    "Question: What are feature engineering techniques that are good to use for predicting the SalePrice of a house?\n",
    "Concept: Housing price predictive model features\n",
    "\n",
    "4) Do not ask questions about tasks that are beyond the scope of my skills because they are too hard for me to do.\n",
    "\n",
    "RESPONSE FORMAT: \n",
    "```json\n",
    "{{ \n",
    "    \"reasoning\": \"<reasoning>\",\n",
    "    \"1\" : {{\n",
    "        \"question\": \"<question>\",\n",
    "        \"concept\": \"<concept>\"\n",
    "    }},\n",
    "    \"2\" : {{\n",
    "        \"question\": \"<question>\",\n",
    "        \"concept\": \"<concept>\"\n",
    "    }},\n",
    "    \"3\" : {{\n",
    "        \"question\": \"<question>\",\n",
    "        \"concept\": \"<concept>\"\n",
    "    }}\n",
    "    ...\n",
    "}}\n",
    "```\n",
    "\n",
    "Ensure the response can be parsed by Python \"json.loads\", e.g.: no trailing commas, no single quotes, etc. This is important.\n",
    "'''\n",
    "\n",
    "        asking_questions_user_prompt = f'''Files: {self.files}\n",
    "Skills: {list(self.available_actions.keys())}    \n",
    "Completed tasks so far: {self.completed_tasks}\n",
    "Failed tasks that are too hard: {self.failed_tasks}\n",
    "Most recent attempted tasks, plans, results, files, and answer states (newest to oldest):\n",
    "{self.formatted_answer_states()}'''\n",
    "        \n",
    "        questions_and_concepts = self.complete_text_openai(system_prompt=asking_questions_system_prompt, user_prompt=asking_questions_user_prompt, json_required=True)\n",
    "        question_and_concepts_json = json.loads(questions_and_concepts) # TODO: potentially add a try and except\n",
    "\n",
    "        # Answer questions\n",
    "        answer_question_system_prompt = f'''You are a helpful assistant that answers my question.\n",
    "        \n",
    "I will give you the following information:\n",
    "Question: ...\n",
    "\n",
    "You will answer the question based on the context (only if available and helpful) and your own knowledge.\n",
    "1) Start your answer with \"Answer: \".\n",
    "2) Answer \"Answer: Unknown\" if you don't know the answer.'''\n",
    "        \n",
    "        # Iterate through question and concepts\n",
    "        question_answer_string = \"\"\n",
    "        for key, value in question_and_concepts_json.items():\n",
    "            if key.isdigit():\n",
    "                # TODO: Use concepts to retrieve from Wikipedia\n",
    "                answer_question_user_prompt = f'''Question: {value['question']}'''\n",
    "                answer = self.complete_text_openai(system_prompt=answer_question_system_prompt, user_prompt=answer_question_user_prompt)\n",
    "\n",
    "                question_answer_string += f\"\\nQuestion {str(key)}: {value['question']}\\n{answer}\"\n",
    "\n",
    "        return question_answer_string\n",
    "\n",
    "    def propose_next_task(self):\n",
    "        '''\n",
    "        This function decomposes a goal into tasks\n",
    "        '''        \n",
    "        question_answer = self.retrieve_from_wiki()\n",
    "        user_prompt = f'''{question_answer}\n",
    "Files: {self.files}\n",
    "Skills: {list(self.available_actions.keys())}    \n",
    "Completed tasks so far: {self.completed_tasks}\n",
    "Failed tasks that are too hard: {self.failed_tasks}\n",
    "Most recent attempted tasks, plans, results, files, and answer states (newest to oldest):\n",
    "{self.formatted_answer_states()}''' # TODO: Should I add formatted_action_history which includes tactical steps that were taken?\n",
    "        \n",
    "        print(\"System prompt for generating curriculum: \\n\", self.system_prompt_automatic_curriculum, \"\\n User prompt: \", user_prompt)\n",
    "        next_task_response = self.complete_text_openai(system_prompt=self.system_prompt_automatic_curriculum, user_prompt=user_prompt, json_required=True)\n",
    "        print(\"Response: \", next_task_response)\n",
    "        next_task = json.loads(next_task_response)[\"task\"]\n",
    "        return next_task\n",
    "\n",
    "    def get_completed_tasks():\n",
    "        pass\n",
    "\n",
    "    def get_failed_tasks():\n",
    "        pass\n",
    "\n",
    "    def add_completed_task(self, task, methods_prompt, result):\n",
    "        # TODO: probably we should record the entire answer state of files, action, output, and answer state? Or just action and output?\n",
    "        self.completed_tasks.insert(0, task)\n",
    "\n",
    "        # Experimenting with adding the task to a living skill library in workspace so the methods prompt can build off of the skills library. \n",
    "\n",
    "        # Asking GPT to write a short file name, and then write the task + methods prompt to the file.\n",
    "        res = self.complete_text_openai(system_prompt=\"You are a helpful assistant that writes a file name of the given task where the file contains a plan for potentially how to achieve that task. The file name should be less than 50 chars. Do not include the extension for the file name, .txt will be automatically added to the end. Your response should be only the file name.\", user_prompt=f\"Task: {task}\")\n",
    "        sanitized_file_name = self.sanitize_filename(res)\n",
    "        with open(self.work_dir + \"/skill_library/\" + sanitized_file_name, \"w\") as f:\n",
    "            f.write(f\"Task: {task}\\n\")\n",
    "            f.write(f\"\\nInstructions: {methods_prompt}\")\n",
    "\n",
    "        # Considering maintaining a running skill library, but adding files is likely not the way to build a wiki otherwise there will likely be a lot of overlapping information? Or not unless they're actually used in the skill library?\n",
    "        with open(self.work_dir.split(\"_branch\")[0] + \"/skill_library/\" + sanitized_file_name, \"w\") as f:\n",
    "            f.write(f\"Task: {task}\\n\")\n",
    "            f.write(f\"\\nInstructions: {methods_prompt}\")\n",
    "\n",
    "        # Update answer state\n",
    "        self.update_answer_state(task, methods_prompt, result)\n",
    "\n",
    "    def add_failed_task(self, task, methods_prompt, result):\n",
    "        self.failed_tasks.insert(0, task) #  + \" \\nCritique for why it failed: \" + critique -- commented this out for now to allow for all tasks to be considered by the curriculum agent without truncation\n",
    "\n",
    "        # Update answer state\n",
    "        self.update_answer_state(task, methods_prompt, result)\n",
    "\n",
    "    def sanitize_filename(self, text):\n",
    "        # Remove invalid file name characters, replace spaces with underscores, lowercase and trim to 50 characters\n",
    "        sanitized = re.sub(r'[^\\w\\s-]', '', text)  # Remove non-word characters except for spaces and hyphens\n",
    "        sanitized = re.sub(r'\\s+', '_', sanitized).strip()[:50].lower()  # Replace spaces, trim, and lower case\n",
    "        return sanitized + '.txt'\n",
    "\n",
    "\n",
    "curriculum_agent = CurriculumAgent(env)\n",
    "# next_task = curriculum_agent.propose_next_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available functions: \n",
      " [{'type': 'function', 'function': {'name': 'reflection', 'description': 'Use this to reflect on all past steps. Provide a detailed description on what to reflect on and what should be returned.', 'parameters': {'type': 'object', 'properties': {'things_to_reflect_on': {'type': 'string', 'description': 'A detailed description on what to reflect on and what should be returned'}}, 'required': ['things_to_reflect_on']}}}, {'type': 'function', 'function': {'name': 'readFile', 'description': 'Use this to read an existing file.', 'parameters': {'type': 'object', 'properties': {'file_name': {'type': 'string', 'description': 'A valid file name with relative path to current directory if needed'}}, 'required': ['file_name']}}}, {'type': 'function', 'function': {'name': 'writeFile', 'description': 'Use this to write a file. If the file already exists, it will be overwritten.', 'parameters': {'type': 'object', 'properties': {'file_name': {'type': 'string', 'description': 'A valid file name with relative path to current directory if needed'}, 'content': {'type': 'string', 'description': 'The content to be written to the file. Please know that the execute script function will execute from the same current working directory. Also that execute script will only output the stdout of the script, so do not use visualizations or other outputs that are not stdout. Be sure to include the file extension.'}}, 'required': ['file_name', 'content']}}}, {'type': 'function', 'function': {'name': 'executeScript', 'description': 'Use this to execute the python script. The script must already exist.', 'parameters': {'type': 'object', 'properties': {'script_name': {'type': 'string', 'description': 'A valid python script name with relative path to current directory if needed. You can only execute scripts and files in the current directory.'}}, 'required': ['script_name']}}}] \n",
      "\n",
      "Functions ['reflection', 'readFile', 'writeFile', 'executeScript'] \n",
      "\n",
      "Available files: \n",
      " ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "class SkillManager(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def retrieve_skills(self, task, execution_feedback):\n",
    "        # For sake of simplicity, use recency for now (later relevancy and importance can be added). TODO: leverage task and execution feedback to guide what action_history memories to retrieve\n",
    "        func_name_description_list = list(self.available_actions.keys())\n",
    "        return func_name_description_list\n",
    "    \n",
    "    # Taking this out for now to just leave the base actions intact because other actions like training a model will be a file that can be retrieved as part of memory for the curriculum agent and not the skill agent\n",
    "    # def retrieve_info_blocks(self, task, execution_feedback):\n",
    "    #     # retrieving file names and description\n",
    "    #     return [name + \" - \" + description for name, description in self.files]\n",
    "\n",
    "    # Helper function: be able to create a function with a dynamic name and return value\n",
    "    # def create_skill_function(self, function_name, return_value):\n",
    "    #     def dynamic_method(self):\n",
    "    #         return return_value\n",
    "    #     # Bind the function to the instance as a method\n",
    "    #     bound_method = types.MethodType(dynamic_method, self)\n",
    "    #     setattr(self, function_name, bound_method)\n",
    "    #     # Add the method to available functions\n",
    "    #     self.available_functions[function_name] = bound_method\n",
    "\n",
    "    # Core function: adding a new skill requires an original task, a validated answer, and a message history\n",
    "    # def add_skill(self, task, validated_answer, methods_prompt):\n",
    "    #     # TODO: wait until the action agent generates a function because maybe you only need to write a description of the input function instead of task and validated answer.\n",
    "    #     # create_function_description_system_prompt = f'''You are a helpful assistant that writes a description of the given '''\n",
    "\n",
    "    #     print(\"Adding skills! \", task, validated_answer, methods_prompt)\n",
    "\n",
    "    #     create_skill_system_prompt = f'''You are a helpful assistant. Your goal is to write a short file name and a short description of the task and answer. \n",
    "        \n",
    "    #     You will receive this information:\n",
    "    #     Original task or question: ...\n",
    "    #     Answer: ...\n",
    "\n",
    "    #     Do not use any of these file names: {[name for name, _ in self.files]}\n",
    "\n",
    "    #     Your output should be in the following format if function requires arguments:\n",
    "    #     ```json\n",
    "    #     {{\n",
    "    #         \"name\": \"<file_name>\",\n",
    "    #         \"description\": \"<insert question and answer>\"\n",
    "    #     }}\n",
    "    #     ```\n",
    "\n",
    "    #     Good example output:\n",
    "    #     ```json\n",
    "    #     {{\n",
    "    #         \"name\": \"num_dogs_in_bens_family\",\n",
    "    #         \"description\": \"The question was how many dogs are in the family. Ben said that he has 2 dogs in his family.\"\n",
    "    #     }}\n",
    "    #     ```\n",
    "\n",
    "    #     Ensure the response can be parsed by Python \"json.loads\", e.g.: no trailing commas, no single quotes, etc. This is important.\n",
    "    #     '''\n",
    "\n",
    "    #     create_function_description_prompt = f'''\n",
    "    #     Original task or question: {task}\n",
    "    #     Answer: {validated_answer}\n",
    "    #     '''\n",
    "    #     res, messages = complete_text_openai(prompt=create_function_description_prompt, system_prompt=create_skill_system_prompt, verbose=True)\n",
    "    #     res\n",
    "\n",
    "    #     try:\n",
    "    #         # Load the function description\n",
    "    #         file_name_description = json.loads(res['content'])\n",
    "    #         print(\"file_name_description: \", file_name_description)\n",
    "\n",
    "    #         # Create the function as a method of skill_manager\n",
    "    #         self.write_file(file_name_description['name'], f\"Question: {task}\\nAnswer: {validated_answer}\\nReasoning and Methods: {methods_prompt}\")\n",
    "\n",
    "    #         # Add function to function description list\n",
    "    #         self.files.append((file_name_description['name'], file_name_description['description']))\n",
    "\n",
    "    #         print(\"COMPLETE!\")\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"An unexpected error occurred: {e}\")\n",
    "    #         return\n",
    "\n",
    "    #     return\n",
    "    \n",
    "skill_manager = SkillManager(env)\n",
    "print(\"Available functions: \\n\", skill_manager.tool_descriptions, \"\\n\\nFunctions\", list(skill_manager.available_actions.keys()), \"\\n\\nAvailable files: \\n\", skill_manager.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodsAgent(Agent):\n",
    "    # TODO: does there need to be an action agent generating the steps? Or does there need to be a separate execution agent running the prompt? Or can the action agent be the execution agent?\n",
    "    # TODO: can a critic agent really check if the output is correct? Or can they only check if that aligns with expectation? Otherwise, the critic will have to check the line of content values to make sure the reasoning is sound, which is still doable, but the extent that another critic can check is limited. I guess it's just to make sure the reasoning is sound.\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def generate_function_callable_prompt(self, task,\n",
    "                methods_prompt,\n",
    "                execution_feedback,\n",
    "                execution_errors,\n",
    "                critique,\n",
    "                skills):\n",
    "        generate_plan_system_prompt = '''You are a helpful assistant and a first-rate problem solver. Given a task or question, your goal is to list out the steps to solve that task given your skills and reasoning. Ultimately, your output should be able to be followed by a human limited by the skills and knowledge given, and another human should be able to check that human's output to see if it's correct and reasonable. Note that the functions asked for may sometimes already be called and the information from the function that you need is already in the prompt, so read carefully. Note that you DO NOT have the ability to see, you can only read, think, write, and execute scripts using the existing skills and knowledge.\n",
    "\n",
    "You will be given this information:\n",
    "Task or question: ...\n",
    "Skills: these are skills that I can take action with.\n",
    "Files: these are my current files that I have in my working directory.  \n",
    "Current state plan: ...\n",
    "Current state output after executing steps: ...\n",
    "Execution errors: ...\n",
    "Critique: ...\n",
    "History of files, action, and result (newest to oldest): By following the plan, this is my history of files, action, and result I had and took at that point in time. \n",
    "\n",
    "You should then respond to me with\n",
    "Explain (if applicable): Are there any steps missing in your plan? Why do the current state steps not complete the task? What do the current state output, execution errors, and critique imply?\n",
    "Plan: How to complete the task step by step. You should pay attention and read Files because it tells you what information you have. The task completeness check is also based on your final action results and final files.\n",
    "Steps: \n",
    "1) Write how to complete the task step by step. \n",
    "2) Reuse the above useful files as much as possible.\n",
    "3) Your task completion and plan will be reused to achieving more complex tasks. Thereofre, you should make it generic and reusable. You should not make strong assumptions about the files (as it may be changed at a later time), and therefore you should always check whether you have the required files before using them. If not, you should first create the required files, get the necessary information, and reuse the above useful actions.\n",
    "'''\n",
    "\n",
    "        user_prompt = f'''Task: {task}\n",
    "Files: {self.files}\n",
    "Skills: {list(self.available_actions.keys())}  \n",
    "Skills: {skills}\n",
    "Current state plan: {methods_prompt}\n",
    "Current state output after executing steps: {execution_feedback}\n",
    "Execution errors: {execution_errors}\n",
    "Critique: {critique}\n",
    "History of files, action, and result:\n",
    "{self.formatted_action_history()}'''\n",
    "\n",
    "        methods_agent_feedback = self.run_assistant(system_prompt=generate_plan_system_prompt, user_prompt=user_prompt, tool_descriptions=self.read_tool_description)\n",
    "        print(\"methods_agent_feedback\", methods_agent_feedback)\n",
    "\n",
    "        return methods_agent_feedback\n",
    "    \n",
    "methods_agent = MethodsAgent(env)\n",
    "methods_prompt = None\n",
    "execution_feedback = None\n",
    "execution_errors = None\n",
    "critique = None\n",
    "success = False\n",
    "# methods_prompt = methods_agent.generate_function_callable_prompt(task=next_task, methods_prompt=methods_prompt, execution_feedback=execution_feedback, execution_errors=execution_errors, critique=critique, skills=list(skill_manager.available_actions.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExecutionAgent(Agent):\n",
    "    # Ultimately the answer will go into Description or be \"returned\" with the description being condensed into a tldr and the methods_prompt will be added to history\n",
    "\n",
    "    # TODO: Perhaps the execution agent can get info about the files too, but maybe that's the action agent's responsibility.\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def function_call(self, task, methods_prompt):\n",
    "        system_prompt = f'''You are a helpful assistant. Your goal is to execute the given instructions and output the complete answer to the question. If the instructions don't seem reasonable or you cannot get to the complete answer, then you should give feedback on why you couldn't do it and what you tried. \n",
    "\n",
    "You will be given this information:\n",
    "Skills: these are skills that I can take action with.\n",
    "Files: these are my current files that I have in my working directory.\n",
    "Task: ...\n",
    "Instructions: ...\n",
    "History of files, action, and result (newest to oldest): By following the plan, this is my history of files, action, and result I had and took at that point in time.'''\n",
    "        execute_prompt = f'''Skills: {list(self.available_actions.keys())}\n",
    "Files: {self.files}\n",
    "Task: {task}\n",
    "Instructions: {methods_prompt}\n",
    "History of files, action, and result:\n",
    "{self.formatted_action_history()}'''\n",
    "\n",
    "        try:\n",
    "            # complete_text_openai(system_prompt=system_prompt, prompt=execute_prompt, tools=self.tool_descriptions, available_functions=self.available_actions) # Normal function calling\n",
    "            self.run_assistant(system_prompt=system_prompt, user_prompt=execute_prompt) # OpenAI Assistants API\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            return \"\"\n",
    "        \n",
    "        return self.formatted_action_history() # Difficult to manage a start index for only answers because some actions get popped or repeated\n",
    "execution_agent = ExecutionAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution_feedback = execution_agent.function_call(methods_prompt=methods_prompt, task=next_task)\n",
    "# print(\"Execution feedback\", execution_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticAgent(Agent):\n",
    "    # TODO: Perhaps include the skills to the critic so the critic knows the facts to check if this makes sense or not\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def check_task_success(self, task, methods_prompt, execution_feedback):\n",
    "        # took out \"Approach: My plan and reasoning to achieve the task.\" because critic agent would say Success to a good plan but incomplete reuslts\n",
    "        system_prompt = '''You are a first-rate researcher that assesses my progress of research and provides useful guidance. \n",
    "        \n",
    "Based on the final files, actions, and results, you are required to evaluate if I have already completed and satisfied all the task requirements. Exceeding the task requirements is also considered a success while failing to complete any of them requires you to provide critique to help me improve and mark my success as False. There must be evidence to show that all the task requirements are already and fully completed for it to be counted as a success. This is important.\n",
    "\n",
    "I will give you the following information:\n",
    "Skills: these are skills that I can take action with.\n",
    "Files: these are my current files that I have in my working directory.\n",
    "Task: The objective I need to accomplish.\n",
    "History of files, action, and result (newest to oldest): After following the plan, this is my history of files, action, and result I had and took at that point in time.\n",
    "\n",
    "You should only respond in JSON format as described below:\n",
    "```json\n",
    "{\n",
    "    \"task\": \"task\",\n",
    "    \"evidence\": \"potential evidence of success\",\n",
    "    \"counter_evidence\": \"potential evidence of failure\",\n",
    "    \"reasoning\": \"reasoning\",\n",
    "    \"success\": boolean,\n",
    "    \"critique\": \"critique\",\n",
    "}\n",
    "```\n",
    "Ensure the response can be parsed by Python \"json.loads\", e.g.: no trailing commas, no single quotes, etc.\n",
    "'''\n",
    "# Commenting out the example because GPT3.5 just inappropriately uses it verbatim it sometimes\n",
    "# RESPONSE:\n",
    "# {\n",
    "#     \"reasoning\": \"The reasoning to get to the answer makes sense, but there's no direct answer for what the actual distribution of the sale price is.\",\n",
    "#     \"success\": False,\n",
    "#     \"critique\": \"The answer only tells us how to get the distribution is, but does not tell us what the actual distribution. Please tell us what the actual distribution is.\",\n",
    "# }\n",
    "\n",
    "        user_prompt = f'''You can only read files to help check if task has been fully completed.\n",
    "Skills: {list(self.available_actions.keys())}\n",
    "Files: {self.files}\n",
    "Task: {task}\n",
    "History of files, action, and result: {self.formatted_action_history()}''' # Execution feedback should be logged in self.formatted_action_history()\n",
    "\n",
    "        # 1. Employing a read assistant first to log files to be checked into file_action_result_history for the critic agent\n",
    "        response_message = self.run_assistant(system_prompt=system_prompt, user_prompt=user_prompt, tool_descriptions=self.read_tool_description)\n",
    "\n",
    "        # 2. Employing a chat completion based on the updated file_action_reuslt_history to make a final judgement\n",
    "        user_prompt = f'''Skills: {list(self.available_actions.keys())}\n",
    "Files: {self.files}\n",
    "Task: {task}\n",
    "History of files, action, and result: {self.formatted_action_history()}''' # Execution feedback should be logged in self.formatted_action_history()\n",
    "        \n",
    "        print(\"Critic system prompt: \", system_prompt, \"\\n\\nCritic user prompt: \", user_prompt, \"\\n\\nTask: \" + task + \"\\n\\nCritic response: \", response_message)\n",
    "\n",
    "        response_message = self.complete_text_openai(system_prompt=system_prompt, user_prompt=response_message, json_required=True)\n",
    "\n",
    "        try:\n",
    "            response_json = json.loads(response_message)\n",
    "            success = response_json['success'] # Must be bool\n",
    "            evidence = str(response_json['evidence'])\n",
    "            opposition = str(response_json['counter_evidence'])\n",
    "            reasoning = str(response_json['reasoning'])\n",
    "            critique = str(response_json['critique'])\n",
    "        except Exception as e:\n",
    "            return False, response_message + \" JSON parsing error: \" + str(e)\n",
    "\n",
    "        # Handle null values\n",
    "        if not reasoning:\n",
    "            reasoning = \"\"\n",
    "        if not critique:\n",
    "            critique = \"\"\n",
    "\n",
    "        return success, \"\\nEvidence: \" + evidence + \"\\nCounter evidence: \" + opposition + \"\\nReasoning: \" + reasoning + \"\\nCritique: \" + critique\n",
    "critic_agent = CriticAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# success, critique = critic_agent.check_task_success(task=next_task, methods_prompt=methods_prompt, execution_feedback=execution_feedback)\n",
    "# print(\"Success: \", success, \"\\nCritique: \", critique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # START HERE FOR NEXT ITERATIONS: Save the current iteration's state after this cycle in case we need to revert\n",
    "# skill_manager_copy = copy.deepcopy(skill_manager)\n",
    "# curriculum_agent_copy = copy.deepcopy(curriculum_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # REVERT BY STARTING HERE, uncomment the below and run\n",
    "# skill_manager = copy.deepcopy(skill_manager_copy)\n",
    "# curriculum_agent = copy.deepcopy(curriculum_agent_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing environment...\n",
      "args namespace(task='home-data-for-ml-course', task_type='kaggle', log_dir='logs/house-price-testing_voyager-v2_gpt4_v4_questioning_wiki', work_dir='workspace', max_steps=50, max_time=18000, device=0, python='/home/user/micromamba/envs/autogpt/bin/python', interactive=False, resume=None, resume_step=0, agent_type='SimpleAssistantAgent', llm_name='gpt-4-1106-preview', fast_llm_name='gpt-4-1106-preview', edit_script_llm_name='gpt-4-1106-preview', edit_script_llm_max_tokens=4000, agent_max_steps=50, actions_remove_from_prompt=[], actions_add_to_prompt=[], no_retrieval=False, valid_format_entires=None, max_steps_in_context=3, max_observation_steps_in_context=3, max_retries=4, langchain_agent='zero-shot-react-description')\n",
      "Preparing task home-data-for-ml-course , of type:  kaggle\n"
     ]
    }
   ],
   "source": [
    "# Running a cycle of Voyager\n",
    "num_rounds = 2 # Just to test if it can give easier tasks too\n",
    "num_tasks = 25\n",
    "\n",
    "env = Environment(args)\n",
    "curriculum_agent = CurriculumAgent(env)\n",
    "skill_manager = SkillManager(env)\n",
    "methods_agent = MethodsAgent(env)\n",
    "execution_agent = ExecutionAgent(env)\n",
    "critic_agent = CriticAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 0 of 25\n",
      "Exploration progress:  Completed tasks: [], Failed tasks: []\n",
      "\n",
      "\n",
      "--- LOGGING NEW ACTION ---\n",
      "Step: 0\n",
      "Calling function wrapped_complete_text_openai(args = (), kwargs = {'system_prompt': 'You are a helpful assistant that asks questions to help me decide the next immediate task to do in research. My ultimate goal is to discover as many useful pieces of information as possible to better achieve the research goal, answer as many questions as possible to get the best answer, and become the best researcher in the world in solving this research goal.\\n\\nResearch Goal: Ask a home buyer to describe their dream house, and they probably won\\'t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition\\'s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\\n\\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\\n\\nEvaluation\\nGoal\\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. You want a train and validation MAE of lower than 15,000 and there should be a submission.csv containing predictions for test.csv ready to submit.\\n\\nMetric\\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\\n\\nSubmission File Format\\nThe file should contain a header and have the following format:\\n\\nId,SalePrice\\n1461,169000.1\\n1462,187724.1233\\n1463,175221\\netc.\\n\\nI will give you the following information:\\nFiles: these are my current files and skills that I have in my working directory.\\nSkills: these are skills that I can take action with.\\nCompleted tasks so far (most recent to least): ...\\nFailed tasks that are too hard (most recent to least): ...\\nMost recent attempted tasks, plans, results, files, and answer states (newest to oldest): Answer states are the report of the best answer I have so far to achieving the research goal, and the attempted tasks, plans, results, and files are the tasks, plans, results, and files I took and had at that point in time to update the answer state.\\n\\nYou must follow the following critiera:\\n1) You should ask at least 5 questions (but no more than 10 questions) to help me decide the next immediate task to do. Each quesiton should be followed by the concept that the question is about.\\n2) You question should be specific to a concept in Wikipedia. The question should not be too general.\\nBad example (the question is too general):\\nQuestion: What is the best way to achieve the research goal?\\nConcept: unknown\\nGood example:\\nQuestion: What are some predictive models that can be used to predict the SalePrice of a house?\\nConcept: housing price predictive model\\n3) Your questions should be self-contained and not require any context.\\nBad example (the question requires the context of my current files):\\nQuestion: Have you checked \\'submission.csv\\' to ensure that the predicted SalePrice values are in a reasonable range compared to the distribution of SalePrice in \\'train.csv\\'?\\nConcept: unknown\\nBad example (the question requires the context of my current files):\\nQuestion: Does the \\'model_training_script.py\\' include a cross-validation process to ensure the model\\'s performance is robust and not overfitting?\\nConcept: unknown\\nGood example: \\nQuestion: What are feature engineering techniques that are good to use for predicting the SalePrice of a house?\\nConcept: Housing price predictive model features\\n\\n4) Do not ask questions about tasks that are beyond the scope of my skills because they are too hard for me to do.\\n\\nRESPONSE FORMAT: \\n```json\\n{ \\n    \"reasoning\": \"<reasoning>\",\\n    \"1\" : {\\n        \"question\": \"<question>\",\\n        \"concept\": \"<concept>\"\\n    },\\n    \"2\" : {\\n        \"question\": \"<question>\",\\n        \"concept\": \"<concept>\"\\n    },\\n    \"3\" : {\\n        \"question\": \"<question>\",\\n        \"concept\": \"<concept>\"\\n    }\\n    ...\\n}\\n```\\n\\nEnsure the response can be parsed by Python \"json.loads\", e.g.: no trailing commas, no single quotes, etc. This is important.\\n', 'user_prompt': \"Files: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\\nSkills: ['reflection', 'readFile', 'writeFile', 'executeScript']    \\nCompleted tasks so far: []\\nFailed tasks that are too hard: []\\nMost recent attempted tasks, plans, results, files, and answer states (newest to oldest):\\n\\nStep: 0\\nAttempted Task: None\\nPlan: None\\nResult: None\\nFiles: ['data_description.txt', 'research_problem.txt', 'sample_submission.csv', 'sample_submission.csv.gz', 'test.csv', 'test.csv.gz', 'train.csv', 'train.csv.gz']\\nAnswer State: None\", 'json_required': True, 'work_dir': 'workspace\\\\home-data-for-ml-course_branch'})\n",
      "\n",
      "--- TOOL SUCCESS ---\n",
      "\n",
      "\n",
      "--- LOGGING NEW ACTION ---\n",
      "Step: 1\n",
      "Calling function wrapped_complete_text_openai(args = (), kwargs = {'system_prompt': 'You are a helpful assistant that answers my question.\\n        \\nI will give you the following information:\\nQuestion: ...\\n\\nYou will answer the question based on the context (only if available and helpful) and your own knowledge.\\n1) Start your answer with \"Answer: \".\\n2) Answer \"Answer: Unknown\" if you don\\'t know the answer.', 'user_prompt': 'Question: What statistical methods can be used to identify and handle missing data within the dataset?', 'work_dir': 'workspace\\\\home-data-for-ml-course_branch'})\n",
      "\n",
      "--- TOOL SUCCESS ---\n",
      "\n",
      "\n",
      "--- LOGGING NEW ACTION ---\n",
      "Step: 2\n",
      "Calling function wrapped_complete_text_openai(args = (), kwargs = {'system_prompt': 'You are a helpful assistant that answers my question.\\n        \\nI will give you the following information:\\nQuestion: ...\\n\\nYou will answer the question based on the context (only if available and helpful) and your own knowledge.\\n1) Start your answer with \"Answer: \".\\n2) Answer \"Answer: Unknown\" if you don\\'t know the answer.', 'user_prompt': 'Question: Which feature selection techniques are effective in reducing dimensionality while retaining important variables for housing price prediction?', 'work_dir': 'workspace\\\\home-data-for-ml-course_branch'})\n",
      "\n",
      "--- TOOL SUCCESS ---\n",
      "\n",
      "\n",
      "--- LOGGING NEW ACTION ---\n",
      "Step: 3\n",
      "Calling function wrapped_complete_text_openai(args = (), kwargs = {'system_prompt': 'You are a helpful assistant that answers my question.\\n        \\nI will give you the following information:\\nQuestion: ...\\n\\nYou will answer the question based on the context (only if available and helpful) and your own knowledge.\\n1) Start your answer with \"Answer: \".\\n2) Answer \"Answer: Unknown\" if you don\\'t know the answer.', 'user_prompt': 'Question: What are common data preprocessing steps required before fitting a predictive model to housing data?', 'work_dir': 'workspace\\\\home-data-for-ml-course_branch'})\n",
      "\n",
      "--- TOOL SUCCESS ---\n",
      "\n",
      "\n",
      "--- LOGGING NEW ACTION ---\n",
      "Step: 4\n",
      "Calling function wrapped_complete_text_openai(args = (), kwargs = {'system_prompt': 'You are a helpful assistant that answers my question.\\n        \\nI will give you the following information:\\nQuestion: ...\\n\\nYou will answer the question based on the context (only if available and helpful) and your own knowledge.\\n1) Start your answer with \"Answer: \".\\n2) Answer \"Answer: Unknown\" if you don\\'t know the answer.', 'user_prompt': 'Question: What machine learning algorithms are commonly used for regression tasks such as predicting housing prices?', 'work_dir': 'workspace\\\\home-data-for-ml-course_branch'})\n",
      "\n",
      "--- TOOL SUCCESS ---\n",
      "\n",
      "\n",
      "--- LOGGING NEW ACTION ---\n",
      "Step: 5\n",
      "Calling function wrapped_complete_text_openai(args = (), kwargs = {'system_prompt': 'You are a helpful assistant that answers my question.\\n        \\nI will give you the following information:\\nQuestion: ...\\n\\nYou will answer the question based on the context (only if available and helpful) and your own knowledge.\\n1) Start your answer with \"Answer: \".\\n2) Answer \"Answer: Unknown\" if you don\\'t know the answer.', 'user_prompt': 'Question: How is cross-validation performed to estimate the performance of a predictive model?', 'work_dir': 'workspace\\\\home-data-for-ml-course_branch'})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_idx in range(num_tasks):\n",
    "    print(f\"\\nTask {task_idx} of {num_tasks}\")\n",
    "\n",
    "    exploration_progress = curriculum_agent.get_exploration_progress()\n",
    "    print(\"Exploration progress: \", exploration_progress)\n",
    "\n",
    "    next_task = curriculum_agent.propose_next_task()\n",
    "    print(\"next_task\", next_task)\n",
    "\n",
    "    methods_prompt = next_task # First round, methods prompt is the task\n",
    "    execution_feedback = None\n",
    "    execution_errors = None\n",
    "    critique = None\n",
    "    success = False\n",
    "    for i in range (num_rounds):\n",
    "        print(f\"\\nRound {i} Task: \", next_task)\n",
    "\n",
    "        if i != 0: # Don't need to generate methods prompt for the first round\n",
    "            print(\"\\nStarting methods agent\")\n",
    "            methods_agent = MethodsAgent(env)\n",
    "            methods_prompt = methods_agent.generate_function_callable_prompt(task=next_task, methods_prompt=methods_prompt, execution_feedback=execution_feedback, execution_errors=execution_errors, critique=critique, skills=list(skill_manager.available_actions.keys()))\n",
    "            print(\"\\nMethods agent output:\\n\", methods_prompt)\n",
    "\n",
    "        print(\"\\nStarting execution agent\")\n",
    "        execution_agent = ExecutionAgent(env)\n",
    "        execution_feedback = execution_agent.function_call(methods_prompt=methods_prompt, task=next_task)\n",
    "        print(\"\\nExecution agent output: \", execution_feedback)\n",
    "\n",
    "        print(\"\\nStarting critic agent\")\n",
    "        critic_agent = CriticAgent(env)\n",
    "        success, critique = critic_agent.check_task_success(task=next_task, methods_prompt=methods_prompt, execution_feedback=execution_feedback)\n",
    "        print(\"Critic agent output\", \"\\nSuccess: \", success, \"\\nCritique: \", critique)\n",
    "\n",
    "        if success:\n",
    "            break\n",
    "    if success:\n",
    "        curriculum_agent.add_completed_task(next_task, methods_prompt, critique)\n",
    "    else:\n",
    "        curriculum_agent.add_failed_task(next_task, methods_prompt, critique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
