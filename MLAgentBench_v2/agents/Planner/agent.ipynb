{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New working directory:  c:\\Users\\kevihuang\\OneDrive - Microsoft\\Desktop\\projects\\MLAgentBench\\MLAgentBench_v2\\agents\\Planner\\..\\..\\..\n",
      "New Working Directory: c:\\Users\\kevihuang\\OneDrive - Microsoft\\Desktop\\projects\\MLAgentBench\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the new working directory relative to the current working directory\n",
    "new_working_directory = os.path.join(os.getcwd(), '..', '..', '..') # Set to MLAgentBenhc\n",
    "print(\"New working directory: \", new_working_directory)\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(new_working_directory)\n",
    "print(\"New Working Directory:\", os.getcwd()) # Should be ...\\MLAgentBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'helm'\n",
      "Could not load CRFM API key crfm_api_key.txt.\n",
      "[Errno 2] No such file or directory: 'claude_api_key.txt'\n",
      "Could not load anthropic API key claude_api_key.txt.\n"
     ]
    }
   ],
   "source": [
    "# General Environment and Agent imports\n",
    "from MLAgentBench_v2.agents.agent import Agent\n",
    "from types import SimpleNamespace\n",
    "from MLAgentBench_v2.environment import Environment\n",
    "from datetime import date\n",
    "\n",
    "# Agent specific imports\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing environment ---\n",
      "--- Environment args ---\n",
      " namespace(task='ai-mathematical-olympiad-prize', task_type='kaggle', log_dir='logs/2024-05-05__ai-mathematical-olympiad-prize__planner__gpt-3.5-turbo-0125__v0', work_dir='workspace', llm_name='gpt-3.5-turbo-0125', research_problem=\"\\nOverview\\nThe goal of this competition is to create algorithms and models that can solve tricky math problems written in LaTeX format. Your participation will help to advance AI models’ mathematical reasoning skills and drive frontier knowledge.\\n\\nDescription\\nThe ability to reason mathematically is a critical milestone for AI. Mathematical reasoning is the foundation for solving many complex problems, from engineering marvels to intricate financial models. However, current AI capabilities are limited in this area.\\n\\nThe AI Mathematical Olympiad (AIMO) Prize is a new $10mn prize fund to spur the open development of AI models capable of performing as well as top human participants in the International Mathematical Olympiad (IMO).\\nThis competition includes 110 problems similar to an intermediate-level high school math challenge. The Gemma 7B benchmark for these problems is 3/50 on the public and private test sets.\\n\\nThe assessment of AI models' mathematical reasoning skills faces a significant hurdle, the issue of train-test leakage. Models trained on Internet-scale datasets may inadvertently encounter test questions during training, skewing the evaluation process.\\n\\nTo address this challenge, this competition uses a dataset of 110 novel math problems, created by an international team of problem solvers, recognizing the need for a transparent and fair evaluation framework. The dataset encompasses a range of difficulty levels, from simple arithmetic to algebraic thinking and geometric reasoning. This will help to strengthen the benchmarks for assessing AI models' mathematical reasoning skills, without the risk of contamination from training data.\\n\\nThis competition offers an exciting opportunity to benchmark open AI models against each other and foster healthy competition and innovation in the field. By addressing this initial benchmarking problem, you will contribute to advancing AI capabilities and help to ensure that its potential benefits outweigh the risks.\\n\\nJoin us as we work towards a future where AI models’ mathematical reasoning skills are accurately and reliably assessed, driving progress and innovation across industries.\\n\\nEvaluation\\nSubmissions are evaluated on the [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) between their predicted labels and the ground-truth labels. In other words, submissions are ranked by the fraction of predicted labels that exactly match the ground-truth labels.\\n\\nIn this competition, every ground-truth label is an integer between 0 and 999, inclusive.\\n\\nSubmitting\\nYou must submit to this competition using the provided Python evaluation API, which serves test set instances one-by-one in random order. To use the API, follow the template in [this notebook](https://www.kaggle.com/code/ryanholbrook/aimo-submission-example).\\n\")\n",
      "Preparing task ai-mathematical-olympiad-prize , of type:  kaggle\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an environment\n",
    "TASK = 'ai-mathematical-olympiad-prize'\n",
    "MODEL = 'gpt-3.5-turbo-0125'\n",
    "AGENT_NAME = 'planner'\n",
    "VERSION = '0'\n",
    "RESEARCH_PROBLEM = '''\n",
    "Overview\n",
    "The goal of this competition is to create algorithms and models that can solve tricky math problems written in LaTeX format. Your participation will help to advance AI models’ mathematical reasoning skills and drive frontier knowledge.\n",
    "\n",
    "Description\n",
    "The ability to reason mathematically is a critical milestone for AI. Mathematical reasoning is the foundation for solving many complex problems, from engineering marvels to intricate financial models. However, current AI capabilities are limited in this area.\n",
    "\n",
    "The AI Mathematical Olympiad (AIMO) Prize is a new $10mn prize fund to spur the open development of AI models capable of performing as well as top human participants in the International Mathematical Olympiad (IMO).\n",
    "This competition includes 110 problems similar to an intermediate-level high school math challenge. The Gemma 7B benchmark for these problems is 3/50 on the public and private test sets.\n",
    "\n",
    "The assessment of AI models' mathematical reasoning skills faces a significant hurdle, the issue of train-test leakage. Models trained on Internet-scale datasets may inadvertently encounter test questions during training, skewing the evaluation process.\n",
    "\n",
    "To address this challenge, this competition uses a dataset of 110 novel math problems, created by an international team of problem solvers, recognizing the need for a transparent and fair evaluation framework. The dataset encompasses a range of difficulty levels, from simple arithmetic to algebraic thinking and geometric reasoning. This will help to strengthen the benchmarks for assessing AI models' mathematical reasoning skills, without the risk of contamination from training data.\n",
    "\n",
    "This competition offers an exciting opportunity to benchmark open AI models against each other and foster healthy competition and innovation in the field. By addressing this initial benchmarking problem, you will contribute to advancing AI capabilities and help to ensure that its potential benefits outweigh the risks.\n",
    "\n",
    "Join us as we work towards a future where AI models’ mathematical reasoning skills are accurately and reliably assessed, driving progress and innovation across industries.\n",
    "\n",
    "Evaluation\n",
    "Submissions are evaluated on the [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) between their predicted labels and the ground-truth labels. In other words, submissions are ranked by the fraction of predicted labels that exactly match the ground-truth labels.\n",
    "\n",
    "In this competition, every ground-truth label is an integer between 0 and 999, inclusive.\n",
    "\n",
    "Submitting\n",
    "You must submit to this competition using the provided Python evaluation API, which serves test set instances one-by-one in random order. To use the API, follow the template in [this notebook](https://www.kaggle.com/code/ryanholbrook/aimo-submission-example).\n",
    "''' # Manually define the goal for the agent based on Kaggle competition description\n",
    "\n",
    "env = Environment(SimpleNamespace(\n",
    "    task=TASK,\n",
    "    task_type='kaggle',\n",
    "    log_dir=f'logs/{date.today().strftime(\"%Y-%m-%d\")}__{TASK}__{AGENT_NAME}__{MODEL}__v{VERSION}', # date__task__agent__llm__version\n",
    "    work_dir='workspace',\n",
    "    llm_name=MODEL,\n",
    "    research_problem=RESEARCH_PROBLEM\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerAgent(Agent):\n",
    "    '''Agent that creates a high level plan of actions to take, and then a human executes the step of the plan and the planner updates the plan accordingly.'''\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "        # Log a history of messages to use as context for the agent (like in swe-agent)\n",
    "        self.system_template_planner = '''SETTING: You are an autonomous programmer trying to solve a machine learning task as effectively as possible.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "You need to format your output using two fields; discussion and plan.\n",
    "Your output should always include _one_ discussion and _one_plan field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "<insert your discussion here>\n",
    "PLAN\n",
    "1. <insert step 1>\n",
    "2. <insert step 2>\n",
    "3. <insert step 3>\n",
    "...\n",
    "\n",
    "You should include a full plan in the plan section and then wait for an executor observation response before continuing with more discussion and plans. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "'''\n",
    "        self.instance_template_planner = f'''We're currently solving the following task without our workspace. Here's the task text:\n",
    "TASK:\n",
    "{self.research_problem}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "Now, you're going to solve this issue on your own. You're already in a workspace with the inital task files downloaded. You can plan on using any actions for the executor to help you, but you may need to adjust your actions and granularity of your plan based on the executor's observation and quality of their execution.\n",
    "\n",
    "Observation: None\n",
    "'''\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        print(\"--- Starting to run Planner Agent ---\")\n",
    "        \n",
    "        # Load saved history in if it exists\n",
    "        if os.path.exists(os.path.join(self.log_dir, 'history.json')):\n",
    "            with open(os.path.join(self.log_dir, 'history.json'), \"r\") as f:\n",
    "                self.history = json.loads(f.read())\n",
    "            for msg in self.history:\n",
    "                if msg['role'] == \"system\":\n",
    "                    print(f\"SYSTEM_PROMPT:\")\n",
    "                elif msg['role'] == \"user\":\n",
    "                    print(f\"OBSERVATION:\")\n",
    "                elif msg['role'] == \"assistant\":\n",
    "                    print(f'REASONING AND PLAN:')\n",
    "                print(msg['content'])\n",
    "        else:\n",
    "            self.history = [{\"role\": \"system\", \"content\": self.system_template_planner}]\n",
    "\n",
    "        MAX_STEPS = 100 # DEBUG: REMOVE WHEN YOU DON'T WANT TO HARD-CODE\n",
    "        observation = \"\"\n",
    "        for _ in range(MAX_STEPS):\n",
    "            # Select the observation template based on what prior observation was\n",
    "            if self.history[-1][\"role\"] == \"system\":\n",
    "                # Log system prompt\n",
    "                print(f\"SYSTEM PROMPT: \\n{self.system_template_planner}\")\n",
    "                with open(self.main_log_path, \"a\", 1) as log_file:\n",
    "                    log_file.write(f\"SYSTEM PROMPT: \")\n",
    "                    log_file.write(self.system_template_planner)\n",
    "                    log_file.write(\"\\n\")\n",
    "\n",
    "                observation_template = self.instance_template_planner\n",
    "            else:\n",
    "                # Get the observation from the human executor\n",
    "                observation = input(\"Please execute step 1 of the plan and report back your observation (type exit to exit): \")\n",
    "                if observation == \"exit\" or observation == '':\n",
    "                    break # gracefully break while preserving history\n",
    "\n",
    "                observation_template = f\"OBSERVATION: \\n{observation}\"\n",
    "            self.history.append({\"role\": \"user\", \"content\": observation_template})\n",
    "            completion = self.query(self.history, self.model)\n",
    "\n",
    "            # Log completion and history\n",
    "            with open(self.main_log_path, \"a\", 1) as log_file:\n",
    "                log_file.write(f\"\\nOBSERVATION: \\n\")\n",
    "                log_file.write(observation_template)\n",
    "                log_file.write(\"\\n\")\n",
    "                log_file.write(f\"\\nREASONING AND PLAN: \\n\")\n",
    "                log_file.write(completion)\n",
    "                log_file.write(\"\\n\")\n",
    "            self.history.append({\"role\": \"assistant\", \"content\": completion})\n",
    "            with open(os.path.join(self.log_dir, 'history.json'), \"w\", 1) as f:\n",
    "                f.write(json.dumps(self.history, indent=4))\n",
    "\n",
    "            print(f\"\\nOBSERVATION: \\n{observation_template}\")\n",
    "            print(f\"\\nREASONING AND PLAN: \\n{completion}\")\n",
    "            \n",
    "        return\n",
    "\n",
    "agent = PlannerAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting to run Planner Agent ---\n",
      "SYSTEM_PROMPT:\n",
      "SETTING: You are an autonomous programmer trying to solve a machine learning task as effectively as possible.\n",
      "\n",
      "RESPONSE FORMAT:\n",
      "You need to format your output using two fields; discussion and plan.\n",
      "Your output should always include _one_ discussion and _one_plan field EXACTLY as in the following example:\n",
      "DISCUSSION\n",
      "<insert your discussion here>\n",
      "PLAN\n",
      "1. <insert step 1>\n",
      "2. <insert step 2>\n",
      "3. <insert step 3>\n",
      "...\n",
      "\n",
      "You should include a full plan in the plan section and then wait for an executor observation response before continuing with more discussion and plans. Everything you include in the DISCUSSION section will be saved for future reference.\n",
      "\n",
      "OBSERVATION:\n",
      "We're currently solving the following task without our workspace. Here's the task text:\n",
      "TASK:\n",
      "\n",
      "Overview\n",
      "The goal of this competition is to create algorithms and models that can solve tricky math problems written in LaTeX format. Your participation will help to advance AI models’ mathematical reasoning skills and drive frontier knowledge.\n",
      "\n",
      "Description\n",
      "The ability to reason mathematically is a critical milestone for AI. Mathematical reasoning is the foundation for solving many complex problems, from engineering marvels to intricate financial models. However, current AI capabilities are limited in this area.\n",
      "\n",
      "The AI Mathematical Olympiad (AIMO) Prize is a new $10mn prize fund to spur the open development of AI models capable of performing as well as top human participants in the International Mathematical Olympiad (IMO).\n",
      "This competition includes 110 problems similar to an intermediate-level high school math challenge. The Gemma 7B benchmark for these problems is 3/50 on the public and private test sets.\n",
      "\n",
      "The assessment of AI models' mathematical reasoning skills faces a significant hurdle, the issue of train-test leakage. Models trained on Internet-scale datasets may inadvertently encounter test questions during training, skewing the evaluation process.\n",
      "\n",
      "To address this challenge, this competition uses a dataset of 110 novel math problems, created by an international team of problem solvers, recognizing the need for a transparent and fair evaluation framework. The dataset encompasses a range of difficulty levels, from simple arithmetic to algebraic thinking and geometric reasoning. This will help to strengthen the benchmarks for assessing AI models' mathematical reasoning skills, without the risk of contamination from training data.\n",
      "\n",
      "This competition offers an exciting opportunity to benchmark open AI models against each other and foster healthy competition and innovation in the field. By addressing this initial benchmarking problem, you will contribute to advancing AI capabilities and help to ensure that its potential benefits outweigh the risks.\n",
      "\n",
      "Join us as we work towards a future where AI models’ mathematical reasoning skills are accurately and reliably assessed, driving progress and innovation across industries.\n",
      "\n",
      "Evaluation\n",
      "Submissions are evaluated on the [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) between their predicted labels and the ground-truth labels. In other words, submissions are ranked by the fraction of predicted labels that exactly match the ground-truth labels.\n",
      "\n",
      "In this competition, every ground-truth label is an integer between 0 and 999, inclusive.\n",
      "\n",
      "Submitting\n",
      "You must submit to this competition using the provided Python evaluation API, which serves test set instances one-by-one in random order. To use the API, follow the template in [this notebook](https://www.kaggle.com/code/ryanholbrook/aimo-submission-example).\n",
      "\n",
      "\n",
      "INSTRUCTIONS:\n",
      "Now, you're going to solve this issue on your own. You're already in a workspace with the inital task files downloaded. You can plan on using any actions for the executor to help you, but you may need to adjust your actions and granularity of your plan based on the executor's observation and quality of their execution.\n",
      "\n",
      "Observation: None\n",
      "\n",
      "REASONING AND PLAN:\n",
      "DISCUSSION\n",
      "The task at hand involves creating algorithms and models that can solve math problems written in LaTeX format. The evaluation metric is based on the accuracy of predicted labels matching the ground-truth labels, which are integers between 0 and 999. The dataset consists of 110 novel math problems, and the goal is to achieve a high accuracy score on these problems. To address the challenge of train-test leakage, we need to ensure that our model generalizes well to unseen data.\n",
      "\n",
      "PLAN\n",
      "1. Explore the dataset: Load and examine the 110 math problems provided in LaTeX format to understand the structure and complexity of the problems.\n",
      "2. Preprocess the data: Convert the LaTeX format into a format suitable for machine learning models, such as numerical representations or tokenization.\n",
      "3. Split the dataset: Divide the dataset into training and validation sets to train the model and evaluate its performance.\n",
      "4. Choose a machine learning model: Select a model suitable for mathematical reasoning tasks, such as neural networks, decision trees, or support vector machines.\n",
      "5. Train the model: Use the training set to train the selected model on the math problems data.\n",
      "6. Evaluate the model: Use the validation set to assess the model's performance in terms of accuracy on unseen data.\n",
      "7. Optimize the model: Fine-tune hyperparameters, adjust the model architecture, or try different algorithms to improve performance.\n",
      "8. Submit predictions: Use the provided Python evaluation API to submit predictions for the test set instances and evaluate the model's accuracy against the ground-truth labels.\n",
      "9. Iterate and improve: Analyze the model's performance, identify weaknesses, and iterate on the process to enhance the model's mathematical reasoning capabilities.\n",
      "OBSERVATION:\n",
      "OBSERVATION: \n",
      "I looked at it. Here is the train.csv file contents: \"id\",\"problem\",\"answer\" \"229ee8\",\"Let $k, l > 0$ be parameters. The parabola $y = kx^2 - 2kx + l$ intersects the line $y = 4$ at two points $A$ and $B$. These points are distance 6 apart. What is the sum of the squares of the distances from $A$ and $B$ to the origin?\",52 \"246d26\",\"Each of the three-digits numbers $111$ to $999$ is coloured blue or yellow in such a way that the sum of any two (not necessarily different) yellow numbers is equal to a blue number. What is the maximum possible number of yellow numbers there can be?\",250 \"2fc4ad\",\"Let the `sparkle' operation on positive integer $n$ consist of calculating the sum of the digits of $n$ and taking its factorial, e.g. the sparkle of 13 is $4! = 24$. A robot starts with a positive integer on a blackboard, then after each second for the rest of eternity, replaces the number on the board with its sparkle. For some `special' numbers, if they're the first number, then eventually every number that appears will be less than 6. How many such special numbers are there with at most 36 digits?\",702 \"430b63\",\"What is the minimum value of $5x^2+5y^2-8xy$ when $x$ and $y$ range over all real numbers such that $|x-2y| + |y-2x| = 40$?\",800 \"5277ed\",\"There exists a unique increasing geometric sequence of five 2-digit positive integers. What is their sum?\",211 \"739bc9\",\"For how many positive integers $m$ does the equation \\[\\vert \\vert x-1 \\vert -2 \\vert=\\frac{m}{100}\\] have $4$ distinct solutions?\",199 \"82e2a0\",\"Suppose that we roll four 6-sided fair dice with faces numbered 1 to~6. Let $a/b$ be the probability that the highest roll is a 5, where $a$ and $b$ are relatively prime positive integers. Find $a + b$.\",185 \"8ee6f3\",\"The points $\\left(x, y\\right)$ satisfying $((\\vert x + y \\vert - 10)^2 + ( \\vert x - y \\vert - 10)^2)((\\vert x \\vert - 8)^2 + ( \\vert y \\vert - 8)^2) = 0$ enclose a convex polygon. What is the area of this convex polygon?\",320 \"bedda4\",\"Let $ABCD$ be a unit square. Let $P$ be the point on $AB$ such that $|AP| = 1/{20}$ and let $Q$ be the point on $AD$ such that $|AQ| = 1/{24}$. The lines $DP$ and $BQ$ divide the square into four regions. Find the ratio between the areas of the largest region and the smallest region.\",480 \"d7e9c9\",\"A function $f: \\mathbb N \\to \\mathbb N$ satisfies the following two conditions for all positive integers $n$:$f(f(f(n)))=8n-7$ and $f(2n)=2f(n)+1$. Calculate $f(100)$.\",199\n",
      "REASONING AND PLAN:\n",
      "DISCUSSION\n",
      "The train.csv file contains the id, problem in LaTeX format, and the answer for each math problem. The problems vary in complexity and cover a range of mathematical topics such as geometry, algebra, probability, and number theory. The answers are integers between 0 and 999, inclusive. We can use this data to train and evaluate our machine learning model for solving these math problems.\n",
      "\n",
      "PLAN\n",
      "1. Load the train.csv file: Read the contents of the train.csv file to access the problem statements and corresponding answers.\n",
      "2. Preprocess the data:\n",
      "   - Extract the problem statements and answers from the CSV file.\n",
      "   - Convert the LaTeX format of the problems into a format suitable for machine learning models, such as tokenization or numerical representation.\n",
      "3. Split the data:\n",
      "   - Divide the dataset into features (problem statements) and labels (answers).\n",
      "   - Split the data into training and validation sets for model training and evaluation.\n",
      "4. Choose a model:\n",
      "   - Select a machine learning model suitable for solving math problems, considering the complexity and variety of the problem statements.\n",
      "5. Train the model:\n",
      "   - Train the selected model on the training set using the problem statements as input and the answers as the target output.\n",
      "6. Evaluate the model:\n",
      "   - Use the validation set to assess the model's performance in terms of accuracy on unseen data.\n",
      "7. Optimize the model:\n",
      "   - Fine-tune hyperparameters, adjust the model architecture, or try different algorithms to improve performance.\n",
      "8. Submit predictions:\n",
      "   - Use the provided Python evaluation API to submit predictions for the test set instances and evaluate the model's accuracy against the ground-truth labels.\n",
      "9. Iterate and improve:\n",
      "   - Analyze the model's performance, identify weaknesses, and iterate on the process to enhance the model's mathematical reasoning capabilities.\n"
     ]
    }
   ],
   "source": [
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
